2022-05-16 20:26:40 - utils.py[line:263] - INFO: distributed init (rank 0): env://
2022-05-16 20:26:40 - utils.py[line:269] - INFO: Start init
2022-05-16 20:26:40 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2022-05-16 20:26:40 - utils.py[line:279] - INFO: initialized host heming-ng1 as rank 0
single-machine distributed training is initialized.
2022-05-16 20:26:41 - train.py[line:77] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'log_nvidia_smi': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'is_moe': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'num_workers_valid': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 5000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './moe_only_decoder_stage1_checkpoints/2_0.06_2500_1_1e-5', 'restore_file': './moe_only_decoder_stage1_checkpoints/10_0.06_2500_1_1e-5/checkpoint.best_cider_1.2880.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_best_checkpoints': False, 'no_save_optimizer_state': False, 'no_save_optimizer_state_on_training_finished': False, 'symlink_best_and_last_checkpoints': False, 'best_checkpoint_metric': 'cider', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 's3_upload_path': None, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807, 'stats_path': None, 'max_valid_steps': None}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_moe_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_type_embedding=True, all_gather_list_size=16384, alternate_ffn_embed_dim=0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='ofa_moe_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=8, batch_size_valid=8, best_checkpoint_metric='cider', bf16=False, block_wise=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_moe_cross_entropy', cross_self_attention=False, curriculum=0, data='../../dataset/caption_data/caption_stage1_train.tsv,../../dataset/caption_data/caption_val.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_moe_freq=2, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, drop_worst_after=2500, drop_worst_ratio=0.2, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_moe_freq=0, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":1,"max_len_b":16,"no_repeat_ngram_size":3}', eval_bleu=False, eval_cider=True, eval_cider_cached_tokens='../../dataset/caption_data/cider_cached_tokens/coco-valid-words.p', eval_print_samples=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, log_nvidia_smi=False, lr=[1e-05], lr_scheduler='polynomial_decay', max_epoch=2, max_source_positions=1024, max_src_length=80, max_target_positions=1024, max_tgt_length=20, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, moe_eval_capacity_token_fraction=0.25, moe_expert_count=4, moe_expert_ffn_dim=0, moe_freq=0, moe_gate_loss_combine_method='average', moe_gate_loss_transform='none', moe_gate_loss_wt=1.0, moe_gating_use_fp32=True, moe_normalize_expert_grad='', moe_normalize_gate_prob_before_dropping=True, moe_second_expert_policy='all', moe_top1_expert=True, no_best_checkpoints=False, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_save_optimizer_state_on_training_finished=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_bins=1000, num_shards=1, num_workers=0, num_workers_valid=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='./moe_only_decoder_stage1_checkpoints/10_0.06_2500_1_1e-5/checkpoint.best_cider_1.2880.pt', s3_upload_path=None, sample_patch_num=196, save_dir='./moe_only_decoder_stage1_checkpoints/2_0.06_2500_1_1e-5', save_interval=1, save_interval_updates=5000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', scst=False, scst_args='{}', seed=1, selected_cols='0,4,2', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, symlink_best_and_last_checkpoints=False, sync_bn=False, task='caption', tensorboard_logdir=None, threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[4], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_moe_pad_mask=True, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, wandb_project=None, warmup_ratio=0.06, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'caption', 'data': '../../dataset/caption_data/caption_stage1_train.tsv,../../dataset/caption_data/caption_val.tsv', 'selected_cols': '0,4,2', 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 80, 'max_tgt_length': 20, 'code_dict_size': 8192, 'patch_image_size': 480, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'eval_bleu': False, 'eval_cider': True, 'eval_args': '{"beam":1,"max_len_b":16,"no_repeat_ngram_size":3}', 'eval_print_samples': False, 'eval_cider_cached_tokens': '../../dataset/caption_data/cider_cached_tokens/coco-valid-words.p', 'scst': False, 'scst_args': '{}'}, 'criterion': {'_name': 'adjust_label_smoothed_moe_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.2, 'drop_worst_after': 2500, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None, 'moe_gate_loss_wt': 1.0, 'moe_gate_loss_combine_method': 'average', 'moe_gate_loss_transform': 'none'}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05], 'block_wise': False}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.06, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-05-16 20:26:42 - ofa_task.py[line:103] - INFO: source dictionary: 59457 types
2022-05-16 20:26:42 - ofa_task.py[line:104] - INFO: target dictionary: 59457 types
2022-05-16 20:26:43 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2022-05-16 20:26:43 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:3 to store for rank: 0
2022-05-16 20:26:43 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:4 to store for rank: 0
2022-05-16 20:26:46 - train.py[line:108] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MOELayer(
          (gate): Top1Gate(
            (wg): Linear(in_features=768, out_features=4, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
          )
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MOELayer(
          (gate): Top1Gate(
            (wg): Linear(in_features=768, out_features=4, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
          )
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MOELayer(
          (gate): Top1Gate(
            (wg): Linear(in_features=768, out_features=4, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
          )
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2022-05-16 20:26:46 - train.py[line:109] - INFO: task: CaptionTask
2022-05-16 20:26:46 - train.py[line:110] - INFO: model: OFAModel
2022-05-16 20:26:46 - train.py[line:111] - INFO: criterion: AdjustLabelSmoothedMOECrossEntropyCriterion
2022-05-16 20:26:46 - train.py[line:112] - INFO: num. shared model params: 168,080,456 (num. trained: 122,417,480)
2022-05-16 20:26:46 - train.py[line:119] - INFO: num. expert model params: 56669184 (num. trained: 56669184)
local datafile ../../dataset/caption_data/caption_val.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_val.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_val.tsv slice_id 0 row count 5000 total row count 5000
/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.layers.1.moe_layer.gate.wg.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.layers.3.moe_layer.gate.wg.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.layers.5.moe_layer.gate.wg.bias
2022-05-16 20:26:47 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2022-05-16 20:26:47 - utils.py[line:779] - INFO: ***********************CUDA enviroments for all 1 workers***********************
2022-05-16 20:26:47 - utils.py[line:781] - INFO: rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-05-16 20:26:47 - utils.py[line:787] - INFO: ***********************CUDA enviroments for all 1 workers***********************
2022-05-16 20:26:47 - train.py[line:150] - INFO: training on 1 devices (GPUs/TPUs)
2022-05-16 20:26:47 - train.py[line:155] - INFO: max tokens per device = None and max sentences per device = 8
2022-05-16 20:26:47 - trainer.py[line:477] - INFO: Preparing to load checkpoint ./moe_only_decoder_stage1_checkpoints/10_0.06_2500_1_1e-5/checkpoint.best_cider_1.2880.pt
2022-05-16 20:26:51 - adam.py[line:70] - INFO: using FusedAdam
2022-05-16 20:26:51 - trainer.py[line:636] - INFO: Loaded checkpoint ./moe_only_decoder_stage1_checkpoints/10_0.06_2500_1_1e-5/checkpoint.best_cider_1.2880.pt (epoch 4 @ 0 updates)
2022-05-16 20:26:51 - trainer.py[line:658] - INFO: loading train data for epoch 1
local datafile ../../dataset/caption_data/caption_stage1_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_stage1_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_stage1_train.tsv slice_id 0 row count 566747 total row count 566747
slice_id 0 seek offset 0
Total steps 35422, warmup steps 2125, warmup_factor 0.00047058823529411766
2022-05-16 20:27:15 - trainer.py[line:722] - INFO: begin training epoch 1
2022-05-16 20:27:15 - train.py[line:303] - INFO: Start iterating over samples
2022-05-16 20:27:24 - trainer.py[line:945] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-05-16 20:27:33 - trainer.py[line:945] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-05-16 20:27:46 - progress_bar.py[line:272] - INFO: epoch 001:     12 / 17711 loss=3.965, loss_v1=0, loss_v2=0, nll_loss=1.987, ntokens=381.8, nsentences=32, sample_size=381.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.774609, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.246, expert1_balance_top=37.991, expert1_balance_bottom=17.898, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.96, wps=190.4, ups=0.5, wpb=381.8, bsz=32, num_updates=10, lr=4.70588e-08, gnorm=6.126, loss_scale=32, train_wall=31, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=59
2022-05-16 20:28:03 - progress_bar.py[line:272] - INFO: epoch 001:     22 / 17711 loss=3.957, loss_v1=0, loss_v2=0, nll_loss=2, ntokens=378.5, nsentences=32, sample_size=378.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.754503, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.241, expert1_balance_top=38.858, expert1_balance_bottom=17.604, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4, wps=225.6, ups=0.6, wpb=378.5, bsz=32, num_updates=20, lr=9.41176e-08, gnorm=6.002, loss_scale=32, train_wall=17, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=76
2022-05-16 20:28:18 - progress_bar.py[line:272] - INFO: epoch 001:     32 / 17711 loss=4.135, loss_v1=0, loss_v2=0, nll_loss=2.186, ntokens=379.3, nsentences=32, sample_size=379.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.765532, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.243, expert1_balance_top=38.306, expert1_balance_bottom=17.591, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.55, wps=249.3, ups=0.66, wpb=379.3, bsz=32, num_updates=30, lr=1.41176e-07, gnorm=6.694, loss_scale=32, train_wall=15, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=91
2022-05-16 20:28:34 - progress_bar.py[line:272] - INFO: epoch 001:     42 / 17711 loss=4.062, loss_v1=0, loss_v2=0, nll_loss=2.102, ntokens=372.6, nsentences=32, sample_size=372.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.766672, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.245, expert1_balance_top=38.094, expert1_balance_bottom=17.765, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.29, wps=229.8, ups=0.62, wpb=372.6, bsz=32, num_updates=40, lr=1.88235e-07, gnorm=6.262, loss_scale=32, train_wall=16, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=108
2022-05-16 20:28:50 - progress_bar.py[line:272] - INFO: epoch 001:     52 / 17711 loss=4.013, loss_v1=0, loss_v2=0, nll_loss=2.065, ntokens=378.9, nsentences=32, sample_size=378.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.753625, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.24, expert1_balance_top=39.622, expert1_balance_bottom=17.359, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.18, wps=250.5, ups=0.66, wpb=378.9, bsz=32, num_updates=50, lr=2.35294e-07, gnorm=6.055, loss_scale=32, train_wall=15, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=123
2022-05-16 20:29:05 - progress_bar.py[line:272] - INFO: epoch 001:     62 / 17711 loss=3.929, loss_v1=0, loss_v2=0, nll_loss=1.936, ntokens=390, nsentences=32, sample_size=390, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.785399, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.25, expert1_balance_top=37.201, expert1_balance_bottom=17.819, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.83, wps=255.9, ups=0.66, wpb=390, bsz=32, num_updates=60, lr=2.82353e-07, gnorm=6.276, loss_scale=32, train_wall=15, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=138
2022-05-16 20:29:20 - progress_bar.py[line:272] - INFO: epoch 001:     72 / 17711 loss=4.211, loss_v1=0, loss_v2=0, nll_loss=2.287, ntokens=386.8, nsentences=32, sample_size=386.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.750144, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.239, expert1_balance_top=38.708, expert1_balance_bottom=17.903, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.88, wps=252.7, ups=0.65, wpb=386.8, bsz=32, num_updates=70, lr=3.29412e-07, gnorm=6.375, loss_scale=32, train_wall=15, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=153
2022-05-16 20:29:35 - progress_bar.py[line:272] - INFO: epoch 001:     82 / 17711 loss=4.1, loss_v1=0, loss_v2=0, nll_loss=2.138, ntokens=376.1, nsentences=32, sample_size=376.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.778167, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.25, expert1_balance_top=37.853, expert1_balance_bottom=17.934, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.4, wps=246.4, ups=0.66, wpb=376.1, bsz=32, num_updates=80, lr=3.76471e-07, gnorm=6.062, loss_scale=32, train_wall=15, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=169
2022-05-16 20:29:52 - progress_bar.py[line:272] - INFO: epoch 001:     92 / 17711 loss=4.004, loss_v1=0, loss_v2=0, nll_loss=2.066, ntokens=374.6, nsentences=32, sample_size=374.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.749372, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.24, expert1_balance_top=38.786, expert1_balance_bottom=17.764, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.19, wps=231.1, ups=0.62, wpb=374.6, bsz=32, num_updates=90, lr=4.23529e-07, gnorm=5.727, loss_scale=32, train_wall=16, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=185
2022-05-16 20:30:15 - progress_bar.py[line:272] - INFO: epoch 001:    102 / 17711 loss=3.98, loss_v1=0, loss_v2=0, nll_loss=2.015, ntokens=387, nsentences=32, sample_size=387, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.77005, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.248, expert1_balance_top=38.033, expert1_balance_bottom=17.68, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.04, wps=165.8, ups=0.43, wpb=387, bsz=32, num_updates=100, lr=4.70588e-07, gnorm=5.807, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=208
2022-05-16 20:30:37 - progress_bar.py[line:272] - INFO: epoch 001:    112 / 17711 loss=4.056, loss_v1=0, loss_v2=0, nll_loss=2.098, ntokens=382.2, nsentences=32, sample_size=382.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.771372, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.246, expert1_balance_top=37.764, expert1_balance_bottom=17.764, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.28, wps=171.6, ups=0.45, wpb=382.2, bsz=32, num_updates=110, lr=5.17647e-07, gnorm=5.889, loss_scale=32, train_wall=16, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=230
2022-05-16 20:31:10 - progress_bar.py[line:272] - INFO: epoch 001:    122 / 17711 loss=4.129, loss_v1=0, loss_v2=0, nll_loss=2.188, ntokens=392.9, nsentences=32, sample_size=392.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.766291, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.245, expert1_balance_top=38.01, expert1_balance_bottom=18.134, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.56, wps=121.6, ups=0.31, wpb=392.9, bsz=32, num_updates=120, lr=5.64706e-07, gnorm=6.022, loss_scale=32, train_wall=17, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=263
2022-05-16 20:31:49 - progress_bar.py[line:272] - INFO: epoch 001:    132 / 17711 loss=4.055, loss_v1=0, loss_v2=0, nll_loss=2.124, ntokens=395.8, nsentences=32, sample_size=395.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.750966, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.24, expert1_balance_top=39.034, expert1_balance_bottom=17.368, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.36, wps=99.4, ups=0.25, wpb=395.8, bsz=32, num_updates=130, lr=6.11765e-07, gnorm=5.905, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=303
2022-05-16 20:32:30 - progress_bar.py[line:272] - INFO: epoch 001:    142 / 17711 loss=3.961, loss_v1=0, loss_v2=0, nll_loss=2.015, ntokens=376.8, nsentences=32, sample_size=376.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.76667, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.245, expert1_balance_top=38.517, expert1_balance_bottom=17.734, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.04, wps=93.1, ups=0.25, wpb=376.8, bsz=32, num_updates=140, lr=6.58824e-07, gnorm=5.532, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=343
2022-05-16 20:33:12 - progress_bar.py[line:272] - INFO: epoch 001:    152 / 17711 loss=3.966, loss_v1=0, loss_v2=0, nll_loss=1.996, ntokens=374.3, nsentences=32, sample_size=374.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.786819, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.256, expert1_balance_top=37.269, expert1_balance_bottom=17.727, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.99, wps=88.7, ups=0.24, wpb=374.3, bsz=32, num_updates=150, lr=7.05882e-07, gnorm=5.412, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=385
2022-05-16 20:33:53 - progress_bar.py[line:272] - INFO: epoch 001:    162 / 17711 loss=3.967, loss_v1=0, loss_v2=0, nll_loss=2.017, ntokens=378.7, nsentences=32, sample_size=378.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.773464, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.249, expert1_balance_top=37.948, expert1_balance_bottom=17.908, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.05, wps=91.9, ups=0.24, wpb=378.7, bsz=32, num_updates=160, lr=7.52941e-07, gnorm=5.192, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=426
2022-05-16 20:34:33 - progress_bar.py[line:272] - INFO: epoch 001:    172 / 17711 loss=4.048, loss_v1=0, loss_v2=0, nll_loss=2.116, ntokens=380.2, nsentences=32, sample_size=380.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.77084, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.249, expert1_balance_top=38.263, expert1_balance_bottom=17.607, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.33, wps=94.6, ups=0.25, wpb=380.2, bsz=32, num_updates=170, lr=8e-07, gnorm=5.665, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=467
2022-05-16 20:35:15 - progress_bar.py[line:272] - INFO: epoch 001:    182 / 17711 loss=3.977, loss_v1=0, loss_v2=0, nll_loss=2.03, ntokens=379.7, nsentences=32, sample_size=379.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.776126, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.252, expert1_balance_top=37.601, expert1_balance_bottom=17.968, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.08, wps=92, ups=0.24, wpb=379.7, bsz=32, num_updates=180, lr=8.47059e-07, gnorm=4.94, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=508
2022-05-16 20:35:54 - progress_bar.py[line:272] - INFO: epoch 001:    192 / 17711 loss=3.934, loss_v1=0, loss_v2=0, nll_loss=1.989, ntokens=379, nsentences=32, sample_size=379, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.773584, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.252, expert1_balance_top=38.083, expert1_balance_bottom=17.746, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.97, wps=97.1, ups=0.26, wpb=379, bsz=32, num_updates=190, lr=8.94118e-07, gnorm=5.104, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=547
2022-05-16 20:36:35 - progress_bar.py[line:272] - INFO: epoch 001:    202 / 17711 loss=3.974, loss_v1=0, loss_v2=0, nll_loss=2.04, ntokens=381.3, nsentences=32, sample_size=381.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.772365, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.252, expert1_balance_top=38.019, expert1_balance_bottom=17.775, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.11, wps=91.9, ups=0.24, wpb=381.3, bsz=32, num_updates=200, lr=9.41176e-07, gnorm=5.005, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=588
2022-05-16 20:37:16 - progress_bar.py[line:272] - INFO: epoch 001:    212 / 17711 loss=3.952, loss_v1=0, loss_v2=0, nll_loss=2.004, ntokens=381.9, nsentences=32, sample_size=381.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.783982, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.257, expert1_balance_top=36.856, expert1_balance_bottom=17.972, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.01, wps=92.8, ups=0.24, wpb=381.9, bsz=32, num_updates=210, lr=9.88235e-07, gnorm=4.82, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=630
2022-05-16 20:37:58 - progress_bar.py[line:272] - INFO: epoch 001:    222 / 17711 loss=3.961, loss_v1=0, loss_v2=0, nll_loss=2.031, ntokens=376.1, nsentences=32, sample_size=376.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.771845, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.252, expert1_balance_top=38.28, expert1_balance_bottom=17.817, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.09, wps=91.3, ups=0.24, wpb=376.1, bsz=32, num_updates=220, lr=1.03529e-06, gnorm=4.27, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=671
2022-05-16 20:38:38 - progress_bar.py[line:272] - INFO: epoch 001:    232 / 17711 loss=3.892, loss_v1=0, loss_v2=0, nll_loss=1.965, ntokens=383.9, nsentences=32, sample_size=383.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.761128, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.248, expert1_balance_top=38.512, expert1_balance_bottom=17.653, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.9, wps=94.2, ups=0.25, wpb=383.9, bsz=32, num_updates=230, lr=1.08235e-06, gnorm=4.706, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=712
2022-05-16 20:39:21 - progress_bar.py[line:272] - INFO: epoch 001:    242 / 17711 loss=3.8, loss_v1=0, loss_v2=0, nll_loss=1.866, ntokens=386.4, nsentences=32, sample_size=386.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.760519, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.249, expert1_balance_top=38.779, expert1_balance_bottom=17.546, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.64, wps=90.9, ups=0.24, wpb=386.4, bsz=32, num_updates=240, lr=1.12941e-06, gnorm=4.052, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=754
2022-05-16 20:40:03 - progress_bar.py[line:272] - INFO: epoch 001:    252 / 17711 loss=3.934, loss_v1=0, loss_v2=0, nll_loss=2, ntokens=382.8, nsentences=32, sample_size=382.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.770154, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.252, expert1_balance_top=38.316, expert1_balance_bottom=17.87, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4, wps=90.5, ups=0.24, wpb=382.8, bsz=32, num_updates=250, lr=1.17647e-06, gnorm=4.121, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=796
2022-05-16 20:40:45 - progress_bar.py[line:272] - INFO: epoch 001:    262 / 17711 loss=4.005, loss_v1=0, loss_v2=0, nll_loss=2.093, ntokens=382.7, nsentences=32, sample_size=382.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.757957, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.247, expert1_balance_top=38.875, expert1_balance_bottom=17.479, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.27, wps=91, ups=0.24, wpb=382.7, bsz=32, num_updates=260, lr=1.22353e-06, gnorm=4.753, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=838
2022-05-16 20:41:27 - progress_bar.py[line:272] - INFO: epoch 001:    272 / 17711 loss=4.059, loss_v1=0, loss_v2=0, nll_loss=2.125, ntokens=376.1, nsentences=32, sample_size=376.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.783273, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.257, expert1_balance_top=37.752, expert1_balance_bottom=17.727, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.36, wps=90.4, ups=0.24, wpb=376.1, bsz=32, num_updates=270, lr=1.27059e-06, gnorm=4.427, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=880
2022-05-16 20:42:09 - progress_bar.py[line:272] - INFO: epoch 001:    282 / 17711 loss=3.898, loss_v1=0, loss_v2=0, nll_loss=1.979, ntokens=383.1, nsentences=32, sample_size=383.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.758265, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.248, expert1_balance_top=39.384, expert1_balance_bottom=17.498, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.94, wps=91.4, ups=0.24, wpb=383.1, bsz=32, num_updates=280, lr=1.31765e-06, gnorm=4.026, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=922
2022-05-16 20:42:52 - progress_bar.py[line:272] - INFO: epoch 001:    292 / 17711 loss=4.046, loss_v1=0, loss_v2=0, nll_loss=2.109, ntokens=371.7, nsentences=32, sample_size=371.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.788735, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.262, expert1_balance_top=37.124, expert1_balance_bottom=18.178, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.31, wps=85.9, ups=0.23, wpb=371.7, bsz=32, num_updates=290, lr=1.36471e-06, gnorm=4.267, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=965
2022-05-16 20:43:33 - progress_bar.py[line:272] - INFO: epoch 001:    302 / 17711 loss=3.936, loss_v1=0, loss_v2=0, nll_loss=2.06, ntokens=386.4, nsentences=32, sample_size=386.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.722328, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.239, expert1_balance_top=40.905, expert1_balance_bottom=17.001, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.17, wps=94, ups=0.24, wpb=386.4, bsz=32, num_updates=300, lr=1.41176e-06, gnorm=4.093, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1006
2022-05-16 20:44:15 - progress_bar.py[line:272] - INFO: epoch 001:    312 / 17711 loss=4.012, loss_v1=0, loss_v2=0, nll_loss=2.097, ntokens=383.4, nsentences=32, sample_size=383.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.766145, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.254, expert1_balance_top=38.259, expert1_balance_bottom=17.59, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.28, wps=91.6, ups=0.24, wpb=383.4, bsz=32, num_updates=310, lr=1.45882e-06, gnorm=3.82, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1048
2022-05-16 20:44:56 - progress_bar.py[line:272] - INFO: epoch 001:    322 / 17711 loss=3.881, loss_v1=0, loss_v2=0, nll_loss=1.964, ntokens=388.7, nsentences=32, sample_size=388.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.751473, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.248, expert1_balance_top=39.755, expert1_balance_bottom=17.256, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.9, wps=95.1, ups=0.24, wpb=388.7, bsz=32, num_updates=320, lr=1.50588e-06, gnorm=3.758, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1089
2022-05-16 20:45:36 - progress_bar.py[line:272] - INFO: epoch 001:    332 / 17711 loss=3.977, loss_v1=0, loss_v2=0, nll_loss=2.08, ntokens=378.8, nsentences=32, sample_size=378.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.748875, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.25, expert1_balance_top=39.556, expert1_balance_bottom=17.237, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.23, wps=94.2, ups=0.25, wpb=378.8, bsz=32, num_updates=330, lr=1.55294e-06, gnorm=4.109, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1129
2022-05-16 20:46:18 - progress_bar.py[line:272] - INFO: epoch 001:    342 / 17711 loss=3.963, loss_v1=0, loss_v2=0, nll_loss=2.026, ntokens=386.1, nsentences=32, sample_size=386.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.778891, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.259, expert1_balance_top=37.701, expert1_balance_bottom=18.229, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.07, wps=92, ups=0.24, wpb=386.1, bsz=32, num_updates=340, lr=1.6e-06, gnorm=4.123, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1171
2022-05-16 20:47:00 - progress_bar.py[line:272] - INFO: epoch 001:    352 / 17711 loss=3.893, loss_v1=0, loss_v2=0, nll_loss=1.945, ntokens=381, nsentences=32, sample_size=381, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.788407, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.26, expert1_balance_top=38.086, expert1_balance_bottom=17.648, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.85, wps=90.6, ups=0.24, wpb=381, bsz=32, num_updates=350, lr=1.64706e-06, gnorm=3.793, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1213
2022-05-16 20:47:41 - progress_bar.py[line:272] - INFO: epoch 001:    362 / 17711 loss=3.857, loss_v1=0, loss_v2=0, nll_loss=1.922, ntokens=382.5, nsentences=32, sample_size=382.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.771335, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.255, expert1_balance_top=38.04, expert1_balance_bottom=17.836, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.79, wps=93.7, ups=0.24, wpb=382.5, bsz=32, num_updates=360, lr=1.69412e-06, gnorm=3.763, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1254
2022-05-16 20:48:22 - progress_bar.py[line:272] - INFO: epoch 001:    372 / 17711 loss=3.921, loss_v1=0, loss_v2=0, nll_loss=1.995, ntokens=377.7, nsentences=32, sample_size=377.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.76725, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.256, expert1_balance_top=38.358, expert1_balance_bottom=17.439, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.99, wps=92, ups=0.24, wpb=377.7, bsz=32, num_updates=370, lr=1.74118e-06, gnorm=3.68, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1295
2022-05-16 20:49:03 - progress_bar.py[line:272] - INFO: epoch 001:    382 / 17711 loss=3.86, loss_v1=0, loss_v2=0, nll_loss=1.909, ntokens=379.3, nsentences=32, sample_size=379.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.78084, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.259, expert1_balance_top=37.97, expert1_balance_bottom=17.921, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.76, wps=92, ups=0.24, wpb=379.3, bsz=32, num_updates=380, lr=1.78824e-06, gnorm=3.87, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1336
2022-05-16 20:49:45 - progress_bar.py[line:272] - INFO: epoch 001:    392 / 17711 loss=3.846, loss_v1=0, loss_v2=0, nll_loss=1.922, ntokens=385, nsentences=32, sample_size=385, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.757363, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.25, expert1_balance_top=39.614, expert1_balance_bottom=17.479, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.79, wps=92.5, ups=0.24, wpb=385, bsz=32, num_updates=390, lr=1.83529e-06, gnorm=3.314, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1378
2022-05-16 20:50:27 - progress_bar.py[line:272] - INFO: epoch 001:    402 / 17711 loss=3.995, loss_v1=0, loss_v2=0, nll_loss=2.092, ntokens=383.5, nsentences=32, sample_size=383.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.74933, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.248, expert1_balance_top=39.595, expert1_balance_bottom=17.448, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.26, wps=91.2, ups=0.24, wpb=383.5, bsz=32, num_updates=400, lr=1.88235e-06, gnorm=3.598, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1420
2022-05-16 20:51:08 - progress_bar.py[line:272] - INFO: epoch 001:    412 / 17711 loss=3.847, loss_v1=0, loss_v2=0, nll_loss=1.901, ntokens=383.4, nsentences=32, sample_size=383.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.779394, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.258, expert1_balance_top=37.695, expert1_balance_bottom=17.895, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.74, wps=93.4, ups=0.24, wpb=383.4, bsz=32, num_updates=410, lr=1.92941e-06, gnorm=4.016, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1461
2022-05-16 20:51:48 - progress_bar.py[line:272] - INFO: epoch 001:    422 / 17711 loss=3.88, loss_v1=0, loss_v2=0, nll_loss=1.926, ntokens=377.8, nsentences=32, sample_size=377.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.787001, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.26, expert1_balance_top=37.395, expert1_balance_bottom=18.163, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.8, wps=93.2, ups=0.25, wpb=377.8, bsz=32, num_updates=420, lr=1.97647e-06, gnorm=3.673, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1502
2022-05-16 20:52:31 - progress_bar.py[line:272] - INFO: epoch 001:    432 / 17711 loss=3.836, loss_v1=0, loss_v2=0, nll_loss=1.882, ntokens=381.2, nsentences=32, sample_size=381.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.781233, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.26, expert1_balance_top=37.549, expert1_balance_bottom=18.086, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.69, wps=89.5, ups=0.23, wpb=381.2, bsz=32, num_updates=430, lr=2.02353e-06, gnorm=3.453, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1544
2022-05-16 20:53:13 - progress_bar.py[line:272] - INFO: epoch 001:    442 / 17711 loss=3.932, loss_v1=0, loss_v2=0, nll_loss=1.973, ntokens=381.6, nsentences=32, sample_size=381.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.793283, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.266, expert1_balance_top=36.097, expert1_balance_bottom=18.518, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.93, wps=89.7, ups=0.24, wpb=381.6, bsz=32, num_updates=440, lr=2.07059e-06, gnorm=3.597, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1587
2022-05-16 20:53:55 - progress_bar.py[line:272] - INFO: epoch 001:    452 / 17711 loss=3.902, loss_v1=0, loss_v2=0, nll_loss=1.953, ntokens=377.2, nsentences=32, sample_size=377.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.782854, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.263, expert1_balance_top=37.565, expert1_balance_bottom=18.028, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.87, wps=90.8, ups=0.24, wpb=377.2, bsz=32, num_updates=450, lr=2.11765e-06, gnorm=3.402, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1628
2022-05-16 20:54:37 - progress_bar.py[line:272] - INFO: epoch 001:    462 / 17711 loss=3.807, loss_v1=0, loss_v2=0, nll_loss=1.848, ntokens=389.3, nsentences=32, sample_size=389.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.78211, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.259, expert1_balance_top=38.149, expert1_balance_bottom=17.899, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.6, wps=92.9, ups=0.24, wpb=389.3, bsz=32, num_updates=460, lr=2.16471e-06, gnorm=3.652, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1670
2022-05-16 20:55:18 - progress_bar.py[line:272] - INFO: epoch 001:    472 / 17711 loss=3.956, loss_v1=0, loss_v2=0, nll_loss=1.991, ntokens=384.6, nsentences=32, sample_size=384.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.802366, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.267, expert1_balance_top=36.483, expert1_balance_bottom=18.485, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.98, wps=92.7, ups=0.24, wpb=384.6, bsz=32, num_updates=470, lr=2.21176e-06, gnorm=3.832, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1712
2022-05-16 20:56:00 - progress_bar.py[line:272] - INFO: epoch 001:    482 / 17711 loss=3.842, loss_v1=0, loss_v2=0, nll_loss=1.89, ntokens=384.4, nsentences=32, sample_size=384.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.776232, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.26, expert1_balance_top=38.116, expert1_balance_bottom=17.582, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.71, wps=92.5, ups=0.24, wpb=384.4, bsz=32, num_updates=480, lr=2.25882e-06, gnorm=3.614, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1753
2022-05-16 20:56:42 - progress_bar.py[line:272] - INFO: epoch 001:    492 / 17711 loss=3.922, loss_v1=0, loss_v2=0, nll_loss=2.025, ntokens=398.2, nsentences=32, sample_size=398.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.732408, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.242, expert1_balance_top=40.041, expert1_balance_bottom=17.127, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.07, wps=94.4, ups=0.24, wpb=398.2, bsz=32, num_updates=490, lr=2.30588e-06, gnorm=3.685, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1795
2022-05-16 20:57:24 - progress_bar.py[line:272] - INFO: epoch 001:    502 / 17711 loss=3.891, loss_v1=0, loss_v2=0, nll_loss=1.966, ntokens=381.1, nsentences=32, sample_size=381.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.757831, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.253, expert1_balance_top=39.007, expert1_balance_bottom=17.563, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.91, wps=90.2, ups=0.24, wpb=381.1, bsz=32, num_updates=500, lr=2.35294e-06, gnorm=3.5, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1838
2022-05-16 20:58:06 - progress_bar.py[line:272] - INFO: epoch 001:    512 / 17711 loss=3.939, loss_v1=0, loss_v2=0, nll_loss=1.998, ntokens=380.4, nsentences=32, sample_size=380.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.776155, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.259, expert1_balance_top=37.837, expert1_balance_bottom=17.722, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4, wps=90.9, ups=0.24, wpb=380.4, bsz=32, num_updates=510, lr=2.4e-06, gnorm=3.555, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1879
2022-05-16 20:58:47 - progress_bar.py[line:272] - INFO: epoch 001:    522 / 17711 loss=3.937, loss_v1=0, loss_v2=0, nll_loss=2.017, ntokens=375.7, nsentences=32, sample_size=375.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.757613, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.253, expert1_balance_top=39.556, expert1_balance_bottom=17.142, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.05, wps=91.3, ups=0.24, wpb=375.7, bsz=32, num_updates=520, lr=2.44706e-06, gnorm=3.293, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1921
2022-05-16 20:59:30 - progress_bar.py[line:272] - INFO: epoch 001:    532 / 17711 loss=3.889, loss_v1=0, loss_v2=0, nll_loss=1.935, ntokens=380.5, nsentences=32, sample_size=380.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.78223, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.262, expert1_balance_top=37.722, expert1_balance_bottom=17.849, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.82, wps=88.9, ups=0.23, wpb=380.5, bsz=32, num_updates=530, lr=2.49412e-06, gnorm=3.22, loss_scale=64, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=1963
2022-05-16 21:00:11 - progress_bar.py[line:272] - INFO: epoch 001:    542 / 17711 loss=3.873, loss_v1=0, loss_v2=0, nll_loss=1.927, ntokens=383.4, nsentences=32, sample_size=383.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.776282, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.26, expert1_balance_top=38.137, expert1_balance_bottom=17.686, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.8, wps=93.3, ups=0.24, wpb=383.4, bsz=32, num_updates=540, lr=2.54118e-06, gnorm=3.426, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2004
2022-05-16 21:00:53 - progress_bar.py[line:272] - INFO: epoch 001:    552 / 17711 loss=3.972, loss_v1=0, loss_v2=0, nll_loss=2.029, ntokens=379.5, nsentences=32, sample_size=379.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.779005, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.261, expert1_balance_top=38.531, expert1_balance_bottom=17.424, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.08, wps=89.9, ups=0.24, wpb=379.5, bsz=32, num_updates=550, lr=2.58824e-06, gnorm=3.323, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2047
2022-05-16 21:01:36 - progress_bar.py[line:272] - INFO: epoch 001:    562 / 17711 loss=3.858, loss_v1=0, loss_v2=0, nll_loss=1.917, ntokens=380.5, nsentences=32, sample_size=380.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.764608, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.254, expert1_balance_top=38.48, expert1_balance_bottom=17.606, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.78, wps=90.3, ups=0.24, wpb=380.5, bsz=32, num_updates=560, lr=2.63529e-06, gnorm=3.388, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2089
2022-05-16 21:02:18 - progress_bar.py[line:272] - INFO: epoch 001:    572 / 17711 loss=3.909, loss_v1=0, loss_v2=0, nll_loss=1.958, ntokens=379.5, nsentences=32, sample_size=379.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.779571, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.263, expert1_balance_top=37.705, expert1_balance_bottom=17.483, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.88, wps=89.8, ups=0.24, wpb=379.5, bsz=32, num_updates=570, lr=2.68235e-06, gnorm=3.386, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2131
2022-05-16 21:03:00 - progress_bar.py[line:272] - INFO: epoch 001:    582 / 17711 loss=3.887, loss_v1=0, loss_v2=0, nll_loss=1.944, ntokens=380.2, nsentences=32, sample_size=380.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.771535, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.259, expert1_balance_top=38.743, expert1_balance_bottom=17.823, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.85, wps=90.1, ups=0.24, wpb=380.2, bsz=32, num_updates=580, lr=2.72941e-06, gnorm=3.536, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2173
2022-05-16 21:03:41 - progress_bar.py[line:272] - INFO: epoch 001:    592 / 17711 loss=3.795, loss_v1=0, loss_v2=0, nll_loss=1.831, ntokens=381.7, nsentences=32, sample_size=381.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.779659, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.261, expert1_balance_top=38.002, expert1_balance_bottom=17.632, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.56, wps=92.4, ups=0.24, wpb=381.7, bsz=32, num_updates=590, lr=2.77647e-06, gnorm=3.414, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2215
2022-05-16 21:04:24 - progress_bar.py[line:272] - INFO: epoch 001:    602 / 17711 loss=3.968, loss_v1=0, loss_v2=0, nll_loss=1.994, ntokens=376.8, nsentences=32, sample_size=376.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.805324, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.271, expert1_balance_top=36.622, expert1_balance_bottom=18.262, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.98, wps=87.8, ups=0.23, wpb=376.8, bsz=32, num_updates=600, lr=2.82353e-06, gnorm=3.507, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2257
2022-05-16 21:05:06 - progress_bar.py[line:272] - INFO: epoch 001:    612 / 17711 loss=3.933, loss_v1=0, loss_v2=0, nll_loss=2.003, ntokens=388.9, nsentences=32, sample_size=388.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.761512, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.255, expert1_balance_top=38.626, expert1_balance_bottom=17.382, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.01, wps=93.4, ups=0.24, wpb=388.9, bsz=32, num_updates=610, lr=2.87059e-06, gnorm=3.508, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2299
2022-05-16 21:05:48 - progress_bar.py[line:272] - INFO: epoch 001:    622 / 17711 loss=3.818, loss_v1=0, loss_v2=0, nll_loss=1.848, ntokens=385.5, nsentences=32, sample_size=385.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.787555, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.264, expert1_balance_top=37.357, expert1_balance_bottom=17.96, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.6, wps=92.2, ups=0.24, wpb=385.5, bsz=32, num_updates=620, lr=2.91765e-06, gnorm=3.707, loss_scale=64, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2341
2022-05-16 21:06:28 - progress_bar.py[line:272] - INFO: epoch 001:    632 / 17711 loss=3.849, loss_v1=0, loss_v2=0, nll_loss=1.893, ntokens=381.9, nsentences=32, sample_size=381.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.77384, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.258, expert1_balance_top=38.625, expert1_balance_bottom=17.787, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.71, wps=94, ups=0.25, wpb=381.9, bsz=32, num_updates=630, lr=2.96471e-06, gnorm=3.199, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2382
2022-05-16 21:07:11 - progress_bar.py[line:272] - INFO: epoch 001:    642 / 17711 loss=3.902, loss_v1=0, loss_v2=0, nll_loss=1.951, ntokens=385.2, nsentences=32, sample_size=385.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.778327, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.262, expert1_balance_top=37.688, expert1_balance_bottom=17.772, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.87, wps=90.2, ups=0.23, wpb=385.2, bsz=32, num_updates=640, lr=3.01176e-06, gnorm=3.714, loss_scale=64, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2424
2022-05-16 21:07:53 - progress_bar.py[line:272] - INFO: epoch 001:    652 / 17711 loss=3.86, loss_v1=0, loss_v2=0, nll_loss=1.919, ntokens=385.3, nsentences=32, sample_size=385.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.763326, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.255, expert1_balance_top=39.034, expert1_balance_bottom=17.213, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.78, wps=92.3, ups=0.24, wpb=385.3, bsz=32, num_updates=650, lr=3.05882e-06, gnorm=3.151, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2466
2022-05-16 21:08:34 - progress_bar.py[line:272] - INFO: epoch 001:    662 / 17711 loss=3.847, loss_v1=0, loss_v2=0, nll_loss=1.901, ntokens=391.9, nsentences=32, sample_size=391.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.766867, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.257, expert1_balance_top=38.612, expert1_balance_bottom=17.634, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.73, wps=94.1, ups=0.24, wpb=391.9, bsz=32, num_updates=660, lr=3.10588e-06, gnorm=3.209, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2508
2022-05-16 21:09:17 - progress_bar.py[line:272] - INFO: epoch 001:    672 / 17711 loss=3.857, loss_v1=0, loss_v2=0, nll_loss=1.914, ntokens=374.1, nsentences=32, sample_size=374.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.762921, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.257, expert1_balance_top=38.563, expert1_balance_bottom=17.254, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.77, wps=87.5, ups=0.23, wpb=374.1, bsz=32, num_updates=670, lr=3.15294e-06, gnorm=3.37, loss_scale=64, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2550
2022-05-16 21:09:58 - progress_bar.py[line:272] - INFO: epoch 001:    682 / 17711 loss=3.791, loss_v1=0, loss_v2=0, nll_loss=1.835, ntokens=386.2, nsentences=32, sample_size=386.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.767972, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.257, expert1_balance_top=38.51, expert1_balance_bottom=17.725, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.57, wps=93.8, ups=0.24, wpb=386.2, bsz=32, num_updates=680, lr=3.2e-06, gnorm=3.361, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2592
2022-05-16 21:10:40 - progress_bar.py[line:272] - INFO: epoch 001:    692 / 17711 loss=3.88, loss_v1=0, loss_v2=0, nll_loss=1.938, ntokens=384.2, nsentences=32, sample_size=384.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.767596, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.257, expert1_balance_top=38.304, expert1_balance_bottom=17.383, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.83, wps=91.6, ups=0.24, wpb=384.2, bsz=32, num_updates=690, lr=3.24706e-06, gnorm=3.269, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2634
2022-05-16 21:11:23 - progress_bar.py[line:272] - INFO: epoch 001:    702 / 17711 loss=3.939, loss_v1=0, loss_v2=0, nll_loss=2.001, ntokens=393.1, nsentences=32, sample_size=393.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.765483, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.254, expert1_balance_top=38.827, expert1_balance_bottom=17.623, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4, wps=91.9, ups=0.23, wpb=393.1, bsz=32, num_updates=700, lr=3.29412e-06, gnorm=3.434, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2676
2022-05-16 21:12:06 - progress_bar.py[line:272] - INFO: epoch 001:    712 / 17711 loss=3.872, loss_v1=0, loss_v2=0, nll_loss=1.943, ntokens=386.6, nsentences=32, sample_size=386.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.752076, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.253, expert1_balance_top=39.731, expert1_balance_bottom=17.077, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.85, wps=90.3, ups=0.23, wpb=386.6, bsz=32, num_updates=710, lr=3.34118e-06, gnorm=3.627, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2719
2022-05-16 21:12:48 - progress_bar.py[line:272] - INFO: epoch 001:    722 / 17711 loss=3.908, loss_v1=0, loss_v2=0, nll_loss=1.968, ntokens=391.4, nsentences=32, sample_size=391.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.765274, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.256, expert1_balance_top=39.236, expert1_balance_bottom=17.117, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.91, wps=93.9, ups=0.24, wpb=391.4, bsz=32, num_updates=720, lr=3.38824e-06, gnorm=3.198, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2761
2022-05-16 21:13:30 - progress_bar.py[line:272] - INFO: epoch 001:    732 / 17711 loss=3.82, loss_v1=0, loss_v2=0, nll_loss=1.879, ntokens=383.8, nsentences=32, sample_size=383.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.757161, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.252, expert1_balance_top=39.707, expert1_balance_bottom=17.192, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.68, wps=91.3, ups=0.24, wpb=383.8, bsz=32, num_updates=730, lr=3.43529e-06, gnorm=3.623, loss_scale=64, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2803
2022-05-16 21:13:34 - trainer.py[line:945] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-05-16 21:14:16 - progress_bar.py[line:272] - INFO: epoch 001:    743 / 17711 loss=3.957, loss_v1=0, loss_v2=0, nll_loss=2.059, ntokens=384, nsentences=32, sample_size=384, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.726714, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.24, expert1_balance_top=41.377, expert1_balance_bottom=16.517, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.17, wps=83.6, ups=0.22, wpb=384, bsz=32, num_updates=740, lr=3.48235e-06, gnorm=3.486, loss_scale=32, train_wall=20, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2849
2022-05-16 21:14:57 - progress_bar.py[line:272] - INFO: epoch 001:    753 / 17711 loss=3.79, loss_v1=0, loss_v2=0, nll_loss=1.823, ntokens=386.4, nsentences=32, sample_size=386.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.776873, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.261, expert1_balance_top=38.113, expert1_balance_bottom=17.78, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.54, wps=92.8, ups=0.24, wpb=386.4, bsz=32, num_updates=750, lr=3.52941e-06, gnorm=3.461, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2891
2022-05-16 21:15:39 - progress_bar.py[line:272] - INFO: epoch 001:    763 / 17711 loss=3.887, loss_v1=0, loss_v2=0, nll_loss=1.939, ntokens=381.4, nsentences=32, sample_size=381.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.768744, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.258, expert1_balance_top=38.684, expert1_balance_bottom=17.253, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.83, wps=92.1, ups=0.24, wpb=381.4, bsz=32, num_updates=760, lr=3.57647e-06, gnorm=3.179, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2932
2022-05-16 21:16:20 - progress_bar.py[line:272] - INFO: epoch 001:    773 / 17711 loss=3.881, loss_v1=0, loss_v2=0, nll_loss=1.928, ntokens=380.4, nsentences=32, sample_size=380.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.771696, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.26, expert1_balance_top=37.468, expert1_balance_bottom=18.061, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.81, wps=91.4, ups=0.24, wpb=380.4, bsz=32, num_updates=770, lr=3.62353e-06, gnorm=3.476, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=2974
2022-05-16 21:17:02 - progress_bar.py[line:272] - INFO: epoch 001:    783 / 17711 loss=3.913, loss_v1=0, loss_v2=0, nll_loss=1.967, ntokens=379.7, nsentences=32, sample_size=379.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.769433, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.258, expert1_balance_top=38.606, expert1_balance_bottom=17.64, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.91, wps=91.9, ups=0.24, wpb=379.7, bsz=32, num_updates=780, lr=3.67059e-06, gnorm=3.256, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3015
2022-05-16 21:17:44 - progress_bar.py[line:272] - INFO: epoch 001:    793 / 17711 loss=3.878, loss_v1=0, loss_v2=0, nll_loss=1.953, ntokens=387.4, nsentences=32, sample_size=387.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.744959, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.25, expert1_balance_top=39.996, expert1_balance_bottom=17.012, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.87, wps=91.9, ups=0.24, wpb=387.4, bsz=32, num_updates=790, lr=3.71765e-06, gnorm=3.503, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3057
2022-05-16 21:18:26 - progress_bar.py[line:272] - INFO: epoch 001:    803 / 17711 loss=3.881, loss_v1=0, loss_v2=0, nll_loss=1.94, ntokens=378.2, nsentences=32, sample_size=378.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.762615, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.256, expert1_balance_top=39.55, expert1_balance_bottom=17.477, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.84, wps=89.3, ups=0.24, wpb=378.2, bsz=32, num_updates=800, lr=3.76471e-06, gnorm=3.066, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3099
2022-05-16 21:19:08 - progress_bar.py[line:272] - INFO: epoch 001:    813 / 17711 loss=3.919, loss_v1=0, loss_v2=0, nll_loss=1.979, ntokens=390.1, nsentences=32, sample_size=390.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.765859, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.259, expert1_balance_top=38.4, expert1_balance_bottom=17.734, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.94, wps=94.1, ups=0.24, wpb=390.1, bsz=32, num_updates=810, lr=3.81176e-06, gnorm=3.037, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3141
2022-05-16 21:19:49 - progress_bar.py[line:272] - INFO: epoch 001:    823 / 17711 loss=3.867, loss_v1=0, loss_v2=0, nll_loss=1.925, ntokens=388.2, nsentences=32, sample_size=388.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.761843, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.256, expert1_balance_top=39.058, expert1_balance_bottom=17.48, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.8, wps=94.5, ups=0.24, wpb=388.2, bsz=32, num_updates=820, lr=3.85882e-06, gnorm=2.973, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3182
2022-05-16 21:20:31 - progress_bar.py[line:272] - INFO: epoch 001:    833 / 17711 loss=3.792, loss_v1=0, loss_v2=0, nll_loss=1.826, ntokens=388.7, nsentences=32, sample_size=388.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.77707, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.259, expert1_balance_top=38.527, expert1_balance_bottom=17.395, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.54, wps=92.1, ups=0.24, wpb=388.7, bsz=32, num_updates=830, lr=3.90588e-06, gnorm=3.023, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3224
2022-05-16 21:21:13 - progress_bar.py[line:272] - INFO: epoch 001:    843 / 17711 loss=3.79, loss_v1=0, loss_v2=0, nll_loss=1.84, ntokens=383.1, nsentences=32, sample_size=383.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.759957, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.254, expert1_balance_top=39.964, expert1_balance_bottom=17.113, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.58, wps=92, ups=0.24, wpb=383.1, bsz=32, num_updates=840, lr=3.95294e-06, gnorm=3.178, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3266
2022-05-16 21:21:54 - progress_bar.py[line:272] - INFO: epoch 001:    853 / 17711 loss=3.852, loss_v1=0, loss_v2=0, nll_loss=1.944, ntokens=393.7, nsentences=32, sample_size=393.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.72686, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.242, expert1_balance_top=41.349, expert1_balance_bottom=16.753, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.85, wps=94.6, ups=0.24, wpb=393.7, bsz=32, num_updates=850, lr=4e-06, gnorm=3.495, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3307
2022-05-16 21:22:36 - progress_bar.py[line:272] - INFO: epoch 001:    863 / 17711 loss=3.8, loss_v1=0, loss_v2=0, nll_loss=1.839, ntokens=381.5, nsentences=32, sample_size=381.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.769627, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.257, expert1_balance_top=39.175, expert1_balance_bottom=17.43, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.58, wps=91.7, ups=0.24, wpb=381.5, bsz=32, num_updates=860, lr=4.04706e-06, gnorm=3.245, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3349
2022-05-16 21:23:16 - progress_bar.py[line:272] - INFO: epoch 001:    873 / 17711 loss=3.863, loss_v1=0, loss_v2=0, nll_loss=1.919, ntokens=381.6, nsentences=32, sample_size=381.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.759587, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.253, expert1_balance_top=39.95, expert1_balance_bottom=16.981, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.78, wps=94.6, ups=0.25, wpb=381.6, bsz=32, num_updates=870, lr=4.09412e-06, gnorm=3.149, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3389
2022-05-16 21:23:57 - progress_bar.py[line:272] - INFO: epoch 001:    883 / 17711 loss=3.902, loss_v1=0, loss_v2=0, nll_loss=1.947, ntokens=382.7, nsentences=32, sample_size=382.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.773513, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.259, expert1_balance_top=39.044, expert1_balance_bottom=17.494, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.86, wps=92.7, ups=0.24, wpb=382.7, bsz=32, num_updates=880, lr=4.14118e-06, gnorm=3.09, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3431
2022-05-16 21:24:38 - progress_bar.py[line:272] - INFO: epoch 001:    893 / 17711 loss=3.891, loss_v1=0, loss_v2=0, nll_loss=1.959, ntokens=390.1, nsentences=32, sample_size=390.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.751894, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.247, expert1_balance_top=40.586, expert1_balance_bottom=17.024, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.89, wps=96.3, ups=0.25, wpb=390.1, bsz=32, num_updates=890, lr=4.18824e-06, gnorm=3.419, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3471
2022-05-16 21:25:20 - progress_bar.py[line:272] - INFO: epoch 001:    903 / 17711 loss=3.845, loss_v1=0, loss_v2=0, nll_loss=1.877, ntokens=382, nsentences=32, sample_size=382, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.782457, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.257, expert1_balance_top=39.04, expert1_balance_bottom=17.274, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.67, wps=90, ups=0.24, wpb=382, bsz=32, num_updates=900, lr=4.23529e-06, gnorm=3.192, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3514
2022-05-16 21:26:03 - progress_bar.py[line:272] - INFO: epoch 001:    913 / 17711 loss=3.954, loss_v1=0, loss_v2=0, nll_loss=2.017, ntokens=388.5, nsentences=32, sample_size=388.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.760176, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.252, expert1_balance_top=40.3, expert1_balance_bottom=16.824, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.05, wps=91.2, ups=0.23, wpb=388.5, bsz=32, num_updates=910, lr=4.28235e-06, gnorm=3.473, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3556
2022-05-16 21:26:44 - progress_bar.py[line:272] - INFO: epoch 001:    923 / 17711 loss=3.825, loss_v1=0, loss_v2=0, nll_loss=1.855, ntokens=376.2, nsentences=32, sample_size=376.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.776529, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.261, expert1_balance_top=38.088, expert1_balance_bottom=17.747, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.62, wps=91.7, ups=0.24, wpb=376.2, bsz=32, num_updates=920, lr=4.32941e-06, gnorm=3.061, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3597
2022-05-16 21:27:25 - progress_bar.py[line:272] - INFO: epoch 001:    933 / 17711 loss=3.837, loss_v1=0, loss_v2=0, nll_loss=1.886, ntokens=378.8, nsentences=32, sample_size=378.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.765237, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.254, expert1_balance_top=39.776, expert1_balance_bottom=17.261, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.69, wps=92.2, ups=0.24, wpb=378.8, bsz=32, num_updates=930, lr=4.37647e-06, gnorm=3.372, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3638
2022-05-16 21:28:06 - progress_bar.py[line:272] - INFO: epoch 001:    943 / 17711 loss=3.799, loss_v1=0, loss_v2=0, nll_loss=1.83, ntokens=377.5, nsentences=32, sample_size=377.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.774951, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.258, expert1_balance_top=39.209, expert1_balance_bottom=17.179, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.56, wps=91.7, ups=0.24, wpb=377.5, bsz=32, num_updates=940, lr=4.42353e-06, gnorm=3.276, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3679
2022-05-16 21:28:48 - progress_bar.py[line:272] - INFO: epoch 001:    953 / 17711 loss=3.911, loss_v1=0, loss_v2=0, nll_loss=1.963, ntokens=377.3, nsentences=32, sample_size=377.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.767777, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.253, expert1_balance_top=39.666, expert1_balance_bottom=17.334, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.9, wps=90, ups=0.24, wpb=377.3, bsz=32, num_updates=950, lr=4.47059e-06, gnorm=4.138, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3721
2022-05-16 21:29:30 - progress_bar.py[line:272] - INFO: epoch 001:    963 / 17711 loss=3.862, loss_v1=0, loss_v2=0, nll_loss=1.922, ntokens=386.8, nsentences=32, sample_size=386.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.755882, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.249, expert1_balance_top=40.533, expert1_balance_bottom=16.76, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.79, wps=92.5, ups=0.24, wpb=386.8, bsz=32, num_updates=960, lr=4.51765e-06, gnorm=3.508, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3763
2022-05-16 21:30:11 - progress_bar.py[line:272] - INFO: epoch 001:    973 / 17711 loss=3.749, loss_v1=0, loss_v2=0, nll_loss=1.809, ntokens=397.1, nsentences=32, sample_size=397.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.746329, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.247, expert1_balance_top=40.8, expert1_balance_bottom=16.957, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.51, wps=96.5, ups=0.24, wpb=397.1, bsz=32, num_updates=970, lr=4.56471e-06, gnorm=3.142, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3804
2022-05-16 21:30:53 - progress_bar.py[line:272] - INFO: epoch 001:    983 / 17711 loss=3.82, loss_v1=0, loss_v2=0, nll_loss=1.846, ntokens=382.9, nsentences=32, sample_size=382.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.781929, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.261, expert1_balance_top=39.04, expert1_balance_bottom=17.256, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.59, wps=90.3, ups=0.24, wpb=382.9, bsz=32, num_updates=980, lr=4.61176e-06, gnorm=3.313, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3847
2022-05-16 21:31:36 - progress_bar.py[line:272] - INFO: epoch 001:    993 / 17711 loss=3.831, loss_v1=0, loss_v2=0, nll_loss=1.882, ntokens=385.1, nsentences=32, sample_size=385.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.759829, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.25, expert1_balance_top=39.867, expert1_balance_bottom=17.175, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.68, wps=91.6, ups=0.24, wpb=385.1, bsz=32, num_updates=990, lr=4.65882e-06, gnorm=2.914, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3889
2022-05-16 21:32:17 - progress_bar.py[line:272] - INFO: epoch 001:   1003 / 17711 loss=3.861, loss_v1=0, loss_v2=0, nll_loss=1.902, ntokens=379.8, nsentences=32, sample_size=379.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.768185, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.252, expert1_balance_top=40.091, expert1_balance_bottom=17.087, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.74, wps=91.6, ups=0.24, wpb=379.8, bsz=32, num_updates=1000, lr=4.70588e-06, gnorm=3.079, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3930
2022-05-16 21:32:59 - progress_bar.py[line:272] - INFO: epoch 001:   1013 / 17711 loss=3.855, loss_v1=0, loss_v2=0, nll_loss=1.891, ntokens=378.6, nsentences=32, sample_size=378.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.775915, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.257, expert1_balance_top=39.768, expert1_balance_bottom=17.123, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.71, wps=90.3, ups=0.24, wpb=378.6, bsz=32, num_updates=1010, lr=4.75294e-06, gnorm=3.088, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=3972
2022-05-16 21:33:41 - progress_bar.py[line:272] - INFO: epoch 001:   1023 / 17711 loss=3.892, loss_v1=0, loss_v2=0, nll_loss=1.906, ntokens=373.4, nsentences=32, sample_size=373.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.796765, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.266, expert1_balance_top=37.506, expert1_balance_bottom=17.807, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.75, wps=88.8, ups=0.24, wpb=373.4, bsz=32, num_updates=1020, lr=4.8e-06, gnorm=3.334, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4014
2022-05-16 21:34:23 - progress_bar.py[line:272] - INFO: epoch 001:   1033 / 17711 loss=3.835, loss_v1=0, loss_v2=0, nll_loss=1.867, ntokens=380, nsentences=32, sample_size=380, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.777108, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.257, expert1_balance_top=39.489, expert1_balance_bottom=17.382, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.65, wps=90.5, ups=0.24, wpb=380, bsz=32, num_updates=1030, lr=4.84706e-06, gnorm=3.376, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4056
2022-05-16 21:35:04 - progress_bar.py[line:272] - INFO: epoch 001:   1043 / 17711 loss=3.827, loss_v1=0, loss_v2=0, nll_loss=1.875, ntokens=375.9, nsentences=32, sample_size=375.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.7631, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.252, expert1_balance_top=40.403, expert1_balance_bottom=17.101, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.67, wps=91.5, ups=0.24, wpb=375.9, bsz=32, num_updates=1040, lr=4.89412e-06, gnorm=3.588, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4097
2022-05-16 21:35:46 - progress_bar.py[line:272] - INFO: epoch 001:   1053 / 17711 loss=3.882, loss_v1=0, loss_v2=0, nll_loss=1.913, ntokens=385.2, nsentences=32, sample_size=385.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.778243, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.257, expert1_balance_top=39.704, expert1_balance_bottom=17.223, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.76, wps=92, ups=0.24, wpb=385.2, bsz=32, num_updates=1050, lr=4.94118e-06, gnorm=3.426, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4139
2022-05-16 21:36:26 - progress_bar.py[line:272] - INFO: epoch 001:   1063 / 17711 loss=3.868, loss_v1=0, loss_v2=0, nll_loss=1.926, ntokens=393.9, nsentences=32, sample_size=393.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.751837, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.246, expert1_balance_top=41.2, expert1_balance_bottom=16.713, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.8, wps=97.4, ups=0.25, wpb=393.9, bsz=32, num_updates=1060, lr=4.98824e-06, gnorm=3.679, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4180
2022-05-16 21:37:09 - progress_bar.py[line:272] - INFO: epoch 001:   1073 / 17711 loss=3.987, loss_v1=0, loss_v2=0, nll_loss=2.059, ntokens=377.2, nsentences=32, sample_size=377.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.759653, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.251, expert1_balance_top=40.53, expert1_balance_bottom=16.878, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.17, wps=88.6, ups=0.23, wpb=377.2, bsz=32, num_updates=1070, lr=5.03529e-06, gnorm=3.23, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4222
2022-05-16 21:37:51 - progress_bar.py[line:272] - INFO: epoch 001:   1083 / 17711 loss=3.831, loss_v1=0, loss_v2=0, nll_loss=1.877, ntokens=379, nsentences=32, sample_size=379, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.766162, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.253, expert1_balance_top=40.041, expert1_balance_bottom=17.589, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.67, wps=89.5, ups=0.24, wpb=379, bsz=32, num_updates=1080, lr=5.08235e-06, gnorm=2.963, loss_scale=32, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4265
2022-05-16 21:38:33 - progress_bar.py[line:272] - INFO: epoch 001:   1093 / 17711 loss=3.841, loss_v1=0, loss_v2=0, nll_loss=1.873, ntokens=386.3, nsentences=32, sample_size=386.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.783233, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.259, expert1_balance_top=38.723, expert1_balance_bottom=17.381, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.66, wps=93.8, ups=0.24, wpb=386.3, bsz=32, num_updates=1090, lr=5.12941e-06, gnorm=3.138, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4306
2022-05-16 21:39:14 - progress_bar.py[line:272] - INFO: epoch 001:   1103 / 17711 loss=3.858, loss_v1=0, loss_v2=0, nll_loss=1.9, ntokens=382.1, nsentences=32, sample_size=382.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.77164, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.256, expert1_balance_top=39.556, expert1_balance_bottom=17.052, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.73, wps=93, ups=0.24, wpb=382.1, bsz=32, num_updates=1100, lr=5.17647e-06, gnorm=3.191, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4347
2022-05-16 21:39:56 - progress_bar.py[line:272] - INFO: epoch 001:   1113 / 17711 loss=3.831, loss_v1=0, loss_v2=0, nll_loss=1.884, ntokens=380.8, nsentences=32, sample_size=380.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.757433, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.249, expert1_balance_top=40.976, expert1_balance_bottom=16.937, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.69, wps=89.8, ups=0.24, wpb=380.8, bsz=32, num_updates=1110, lr=5.22353e-06, gnorm=3.439, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4389
2022-05-16 21:40:38 - progress_bar.py[line:272] - INFO: epoch 001:   1123 / 17711 loss=3.831, loss_v1=0, loss_v2=0, nll_loss=1.867, ntokens=376, nsentences=32, sample_size=376, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.771394, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.255, expert1_balance_top=39.579, expert1_balance_bottom=17.448, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.65, wps=89.8, ups=0.24, wpb=376, bsz=32, num_updates=1120, lr=5.27059e-06, gnorm=3.376, loss_scale=32, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4431
2022-05-16 21:40:42 - trainer.py[line:945] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-05-16 21:41:24 - progress_bar.py[line:272] - INFO: epoch 001:   1134 / 17711 loss=3.813, loss_v1=0, loss_v2=0, nll_loss=1.874, ntokens=387.3, nsentences=32, sample_size=387.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.750052, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.246, expert1_balance_top=40.441, expert1_balance_bottom=17.223, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.67, wps=83.6, ups=0.22, wpb=387.3, bsz=32, num_updates=1130, lr=5.31765e-06, gnorm=3.23, loss_scale=16, train_wall=20, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4477
2022-05-16 21:42:07 - progress_bar.py[line:272] - INFO: epoch 001:   1144 / 17711 loss=3.809, loss_v1=0, loss_v2=0, nll_loss=1.852, ntokens=391.1, nsentences=32, sample_size=391.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.763085, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.252, expert1_balance_top=39.581, expert1_balance_bottom=17.126, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.61, wps=91.4, ups=0.23, wpb=391.1, bsz=32, num_updates=1140, lr=5.36471e-06, gnorm=3.246, loss_scale=16, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4520
2022-05-16 21:42:49 - progress_bar.py[line:272] - INFO: epoch 001:   1154 / 17711 loss=3.856, loss_v1=0, loss_v2=0, nll_loss=1.917, ntokens=388.7, nsentences=32, sample_size=388.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.74971, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.246, expert1_balance_top=41.037, expert1_balance_bottom=17.163, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.78, wps=91.6, ups=0.24, wpb=388.7, bsz=32, num_updates=1150, lr=5.41176e-06, gnorm=3.44, loss_scale=16, train_wall=19, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4563
2022-05-16 21:43:31 - progress_bar.py[line:272] - INFO: epoch 001:   1164 / 17711 loss=3.773, loss_v1=0, loss_v2=0, nll_loss=1.797, ntokens=383, nsentences=32, sample_size=383, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.776291, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.257, expert1_balance_top=39.413, expert1_balance_bottom=17.3, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.48, wps=92.3, ups=0.24, wpb=383, bsz=32, num_updates=1160, lr=5.45882e-06, gnorm=3.836, loss_scale=16, train_wall=18, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4604
2022-05-16 21:43:56 - progress_bar.py[line:272] - INFO: epoch 001:   1174 / 17711 loss=3.773, loss_v1=0, loss_v2=0, nll_loss=1.802, ntokens=383.4, nsentences=32, sample_size=383.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.77427, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.254, expert1_balance_top=39.469, expert1_balance_bottom=17.255, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.49, wps=151, ups=0.39, wpb=383.4, bsz=32, num_updates=1170, lr=5.50588e-06, gnorm=3.127, loss_scale=16, train_wall=17, cuda_gb_allocated=11.7, cuda_gb_reserved=12.5, cuda_gb_free=12, wall=4630
Killing subprocess 42467
Main process received SIGINT, exiting
