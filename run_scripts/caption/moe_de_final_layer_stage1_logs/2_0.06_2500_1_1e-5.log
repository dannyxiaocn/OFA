2022-05-16 20:25:54 - utils.py[line:263] - INFO: distributed init (rank 0): env://
2022-05-16 20:25:54 - utils.py[line:269] - INFO: Start init
2022-05-16 20:25:54 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2022-05-16 20:25:54 - utils.py[line:279] - INFO: initialized host heming-ng1 as rank 0
single-machine distributed training is initialized.
2022-05-16 20:25:59 - train.py[line:77] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'log_nvidia_smi': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'is_moe': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'num_workers_valid': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 4000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './moe_de_final_layer_stage1_checkpoints/2_0.06_2500_1_1e-5', 'restore_file': './moe_de_final_layer_stage1_checkpoints/6_0.06_2500_1_1e-5/checkpoint.best_cider_1.2910.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 4000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_best_checkpoints': False, 'no_save_optimizer_state': False, 'no_save_optimizer_state_on_training_finished': False, 'symlink_best_and_last_checkpoints': False, 'best_checkpoint_metric': 'cider', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 's3_upload_path': None, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807, 'stats_path': None, 'max_valid_steps': None}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_moe_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_type_embedding=True, all_gather_list_size=16384, alternate_ffn_embed_dim=0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='ofa_moe_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=8, batch_size_valid=8, best_checkpoint_metric='cider', bf16=False, block_wise=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_moe_cross_entropy', cross_self_attention=False, curriculum=0, data='../../dataset/caption_data/caption_stage1_train.tsv,../../dataset/caption_data/caption_val.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_moe_freq=6, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, drop_worst_after=2500, drop_worst_ratio=0.2, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_moe_freq=0, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":1,"max_len_b":16,"no_repeat_ngram_size":3}', eval_bleu=False, eval_cider=True, eval_cider_cached_tokens='../../dataset/caption_data/cider_cached_tokens/coco-valid-words.p', eval_print_samples=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, log_nvidia_smi=False, lr=[1e-05], lr_scheduler='polynomial_decay', max_epoch=2, max_source_positions=1024, max_src_length=80, max_target_positions=1024, max_tgt_length=20, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, moe_eval_capacity_token_fraction=0.25, moe_expert_count=4, moe_expert_ffn_dim=0, moe_freq=0, moe_gate_loss_combine_method='average', moe_gate_loss_transform='none', moe_gate_loss_wt=1.0, moe_gating_use_fp32=True, moe_normalize_expert_grad='', moe_normalize_gate_prob_before_dropping=True, moe_second_expert_policy='all', moe_top1_expert=True, no_best_checkpoints=False, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_save_optimizer_state_on_training_finished=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_bins=1000, num_shards=1, num_workers=0, num_workers_valid=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='./moe_de_final_layer_stage1_checkpoints/6_0.06_2500_1_1e-5/checkpoint.best_cider_1.2910.pt', s3_upload_path=None, sample_patch_num=196, save_dir='./moe_de_final_layer_stage1_checkpoints/2_0.06_2500_1_1e-5', save_interval=1, save_interval_updates=4000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', scst=False, scst_args='{}', seed=1, selected_cols='0,4,2', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, symlink_best_and_last_checkpoints=False, sync_bn=False, task='caption', tensorboard_logdir=None, threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[4], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_moe_pad_mask=True, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=4000, wandb_project=None, warmup_ratio=0.06, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'caption', 'data': '../../dataset/caption_data/caption_stage1_train.tsv,../../dataset/caption_data/caption_val.tsv', 'selected_cols': '0,4,2', 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 80, 'max_tgt_length': 20, 'code_dict_size': 8192, 'patch_image_size': 480, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'eval_bleu': False, 'eval_cider': True, 'eval_args': '{"beam":1,"max_len_b":16,"no_repeat_ngram_size":3}', 'eval_print_samples': False, 'eval_cider_cached_tokens': '../../dataset/caption_data/cider_cached_tokens/coco-valid-words.p', 'scst': False, 'scst_args': '{}'}, 'criterion': {'_name': 'adjust_label_smoothed_moe_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.2, 'drop_worst_after': 2500, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None, 'moe_gate_loss_wt': 1.0, 'moe_gate_loss_combine_method': 'average', 'moe_gate_loss_transform': 'none'}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05], 'block_wise': False}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.06, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-05-16 20:25:59 - ofa_task.py[line:103] - INFO: source dictionary: 59457 types
2022-05-16 20:25:59 - ofa_task.py[line:104] - INFO: target dictionary: 59457 types
2022-05-16 20:26:01 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2022-05-16 20:26:02 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:3 to store for rank: 0
2022-05-16 20:26:02 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:4 to store for rank: 0
2022-05-16 20:26:04 - train.py[line:108] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MOELayer(
          (gate): Top1Gate(
            (wg): Linear(in_features=768, out_features=4, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
          )
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2022-05-16 20:26:04 - train.py[line:109] - INFO: task: CaptionTask
2022-05-16 20:26:04 - train.py[line:110] - INFO: model: OFAModel
2022-05-16 20:26:04 - train.py[line:111] - INFO: criterion: AdjustLabelSmoothedMOECrossEntropyCriterion
2022-05-16 20:26:04 - train.py[line:112] - INFO: num. shared model params: 177,519,176 (num. trained: 131,856,200)
2022-05-16 20:26:04 - train.py[line:119] - INFO: num. expert model params: 18889728 (num. trained: 18889728)
local datafile ../../dataset/caption_data/caption_val.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_val.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_val.tsv slice_id 0 row count 5000 total row count 5000
/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.layers.5.moe_layer.gate.wg.bias
2022-05-16 20:26:04 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2022-05-16 20:26:04 - utils.py[line:779] - INFO: ***********************CUDA enviroments for all 1 workers***********************
2022-05-16 20:26:04 - utils.py[line:781] - INFO: rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-05-16 20:26:04 - utils.py[line:787] - INFO: ***********************CUDA enviroments for all 1 workers***********************
2022-05-16 20:26:04 - train.py[line:150] - INFO: training on 1 devices (GPUs/TPUs)
2022-05-16 20:26:04 - train.py[line:155] - INFO: max tokens per device = None and max sentences per device = 8
2022-05-16 20:26:04 - trainer.py[line:477] - INFO: Preparing to load checkpoint ./moe_de_final_layer_stage1_checkpoints/6_0.06_2500_1_1e-5/checkpoint.best_cider_1.2910.pt
2022-05-16 20:26:06 - adam.py[line:70] - INFO: using FusedAdam
2022-05-16 20:26:06 - trainer.py[line:636] - INFO: Loaded checkpoint ./moe_de_final_layer_stage1_checkpoints/6_0.06_2500_1_1e-5/checkpoint.best_cider_1.2910.pt (epoch 2 @ 0 updates)
2022-05-16 20:26:06 - trainer.py[line:658] - INFO: loading train data for epoch 1
local datafile ../../dataset/caption_data/caption_stage1_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_stage1_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_stage1_train.tsv slice_id 0 row count 566747 total row count 566747
slice_id 0 seek offset 0
Total steps 35422, warmup steps 2125, warmup_factor 0.00047058823529411766
2022-05-16 20:26:40 - trainer.py[line:722] - INFO: begin training epoch 1
2022-05-16 20:26:40 - train.py[line:303] - INFO: Start iterating over samples
2022-05-16 20:26:52 - trainer.py[line:945] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-05-16 20:27:00 - trainer.py[line:945] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-05-16 20:27:14 - progress_bar.py[line:272] - INFO: epoch 001:     12 / 17711 loss=3.994, loss_v1=0, loss_v2=0, nll_loss=2.008, ntokens=381.8, nsentences=32, sample_size=381.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.774271, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.082, expert1_balance_bottom=16.785, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.02, wps=190.4, ups=0.5, wpb=381.8, bsz=32, num_updates=10, lr=4.70588e-08, gnorm=6.881, loss_scale=32, train_wall=33, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=70
2022-05-16 20:27:30 - progress_bar.py[line:272] - INFO: epoch 001:     22 / 17711 loss=3.998, loss_v1=0, loss_v2=0, nll_loss=2.033, ntokens=378.5, nsentences=32, sample_size=378.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.757246, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.776, expert1_balance_bottom=16.61, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.09, wps=229.4, ups=0.61, wpb=378.5, bsz=32, num_updates=20, lr=9.41176e-08, gnorm=6.39, loss_scale=32, train_wall=16, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=86
2022-05-16 20:27:45 - progress_bar.py[line:272] - INFO: epoch 001:     32 / 17711 loss=4.166, loss_v1=0, loss_v2=0, nll_loss=2.211, ntokens=379.3, nsentences=32, sample_size=379.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.765146, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.699, expert1_balance_bottom=17.195, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.63, wps=253.2, ups=0.67, wpb=379.3, bsz=32, num_updates=30, lr=1.41176e-07, gnorm=7.126, loss_scale=32, train_wall=15, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=101
2022-05-16 20:28:01 - progress_bar.py[line:272] - INFO: epoch 001:     42 / 17711 loss=4.075, loss_v1=0, loss_v2=0, nll_loss=2.106, ntokens=372.6, nsentences=32, sample_size=372.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.767128, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.065, expert1_balance_bottom=17.133, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.31, wps=237.9, ups=0.64, wpb=372.6, bsz=32, num_updates=40, lr=1.88235e-07, gnorm=6.28, loss_scale=32, train_wall=16, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=117
2022-05-16 20:28:16 - progress_bar.py[line:272] - INFO: epoch 001:     52 / 17711 loss=4.035, loss_v1=0, loss_v2=0, nll_loss=2.078, ntokens=378.9, nsentences=32, sample_size=378.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.752894, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.472, expert1_balance_bottom=17.284, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.22, wps=255.7, ups=0.67, wpb=378.9, bsz=32, num_updates=50, lr=2.35294e-07, gnorm=7.026, loss_scale=32, train_wall=15, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=131
2022-05-16 20:28:31 - progress_bar.py[line:272] - INFO: epoch 001:     62 / 17711 loss=3.954, loss_v1=0, loss_v2=0, nll_loss=1.955, ntokens=390, nsentences=32, sample_size=390, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.784883, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.6, expert1_balance_bottom=17.576, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.88, wps=257.2, ups=0.66, wpb=390, bsz=32, num_updates=60, lr=2.82353e-07, gnorm=6.359, loss_scale=32, train_wall=15, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=147
2022-05-16 20:28:46 - progress_bar.py[line:272] - INFO: epoch 001:     72 / 17711 loss=4.252, loss_v1=0, loss_v2=0, nll_loss=2.321, ntokens=386.8, nsentences=32, sample_size=386.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.751799, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.844, expert1_balance_bottom=17.071, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=5, wps=256.5, ups=0.66, wpb=386.8, bsz=32, num_updates=70, lr=3.29412e-07, gnorm=7.433, loss_scale=32, train_wall=15, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=162
2022-05-16 20:29:01 - progress_bar.py[line:272] - INFO: epoch 001:     82 / 17711 loss=4.099, loss_v1=0, loss_v2=0, nll_loss=2.124, ntokens=376.1, nsentences=32, sample_size=376.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.777981, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.16, expert1_balance_bottom=17.671, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.36, wps=250.4, ups=0.67, wpb=376.1, bsz=32, num_updates=80, lr=3.76471e-07, gnorm=6.312, loss_scale=32, train_wall=15, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=177
2022-05-16 20:29:16 - progress_bar.py[line:272] - INFO: epoch 001:     92 / 17711 loss=4.004, loss_v1=0, loss_v2=0, nll_loss=2.052, ntokens=374.6, nsentences=32, sample_size=374.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.752399, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.842, expert1_balance_bottom=16.902, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.15, wps=246.4, ups=0.66, wpb=374.6, bsz=32, num_updates=90, lr=4.23529e-07, gnorm=5.706, loss_scale=32, train_wall=15, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=192
2022-05-16 20:29:33 - progress_bar.py[line:272] - INFO: epoch 001:    102 / 17711 loss=3.995, loss_v1=0, loss_v2=0, nll_loss=2.019, ntokens=387, nsentences=32, sample_size=387, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.771928, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.414, expert1_balance_bottom=16.84, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.05, wps=233.2, ups=0.6, wpb=387, bsz=32, num_updates=100, lr=4.70588e-07, gnorm=6.206, loss_scale=32, train_wall=17, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=209
2022-05-16 20:29:48 - progress_bar.py[line:272] - INFO: epoch 001:    112 / 17711 loss=4.071, loss_v1=0, loss_v2=0, nll_loss=2.104, ntokens=382.2, nsentences=32, sample_size=382.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.77238, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=34.532, expert1_balance_bottom=17.81, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.3, wps=241, ups=0.63, wpb=382.2, bsz=32, num_updates=110, lr=5.17647e-07, gnorm=6.818, loss_scale=32, train_wall=16, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=224
2022-05-16 20:30:05 - progress_bar.py[line:272] - INFO: epoch 001:    122 / 17711 loss=4.126, loss_v1=0, loss_v2=0, nll_loss=2.175, ntokens=392.9, nsentences=32, sample_size=392.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.766494, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.488, expert1_balance_bottom=17.263, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.51, wps=240.6, ups=0.61, wpb=392.9, bsz=32, num_updates=120, lr=5.64706e-07, gnorm=5.933, loss_scale=32, train_wall=16, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=241
2022-05-16 20:30:24 - progress_bar.py[line:272] - INFO: epoch 001:    132 / 17711 loss=4.052, loss_v1=0, loss_v2=0, nll_loss=2.11, ntokens=395.8, nsentences=32, sample_size=395.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.752841, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.577, expert1_balance_bottom=17.269, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.32, wps=203.2, ups=0.51, wpb=395.8, bsz=32, num_updates=130, lr=6.11765e-07, gnorm=5.395, loss_scale=32, train_wall=16, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=260
2022-05-16 20:30:43 - progress_bar.py[line:272] - INFO: epoch 001:    142 / 17711 loss=3.973, loss_v1=0, loss_v2=0, nll_loss=2.018, ntokens=376.8, nsentences=32, sample_size=376.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.76665, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.371, expert1_balance_bottom=17.123, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.05, wps=202.4, ups=0.54, wpb=376.8, bsz=32, num_updates=140, lr=6.58824e-07, gnorm=5.914, loss_scale=32, train_wall=16, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=279
2022-05-16 20:31:16 - progress_bar.py[line:272] - INFO: epoch 001:    152 / 17711 loss=3.978, loss_v1=0, loss_v2=0, nll_loss=2.002, ntokens=374.3, nsentences=32, sample_size=374.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.786298, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.289, expert1_balance_bottom=17.524, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.01, wps=114.5, ups=0.31, wpb=374.3, bsz=32, num_updates=150, lr=7.05882e-07, gnorm=5.2, loss_scale=32, train_wall=17, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=312
2022-05-16 20:31:56 - progress_bar.py[line:272] - INFO: epoch 001:    162 / 17711 loss=3.997, loss_v1=0, loss_v2=0, nll_loss=2.037, ntokens=378.7, nsentences=32, sample_size=378.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.775762, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=34.554, expert1_balance_bottom=17.546, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.1, wps=94.6, ups=0.25, wpb=378.7, bsz=32, num_updates=160, lr=7.52941e-07, gnorm=5.591, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=352
2022-05-16 20:32:38 - progress_bar.py[line:272] - INFO: epoch 001:    172 / 17711 loss=4.05, loss_v1=0, loss_v2=0, nll_loss=2.108, ntokens=380.2, nsentences=32, sample_size=380.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.773724, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.636, expert1_balance_bottom=16.503, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.31, wps=90.6, ups=0.24, wpb=380.2, bsz=32, num_updates=170, lr=8e-07, gnorm=5.509, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=394
2022-05-16 20:33:20 - progress_bar.py[line:272] - INFO: epoch 001:    182 / 17711 loss=4.006, loss_v1=0, loss_v2=0, nll_loss=2.051, ntokens=379.7, nsentences=32, sample_size=379.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.77692, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.104, expert1_balance_bottom=17.949, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.14, wps=88.8, ups=0.23, wpb=379.7, bsz=32, num_updates=180, lr=8.47059e-07, gnorm=4.891, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=436
2022-05-16 20:34:02 - progress_bar.py[line:272] - INFO: epoch 001:    192 / 17711 loss=3.95, loss_v1=0, loss_v2=0, nll_loss=1.994, ntokens=379, nsentences=32, sample_size=379, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.773638, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.073, expert1_balance_bottom=17.058, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.98, wps=90.9, ups=0.24, wpb=379, bsz=32, num_updates=190, lr=8.94118e-07, gnorm=4.771, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=478
2022-05-16 20:34:45 - progress_bar.py[line:272] - INFO: epoch 001:    202 / 17711 loss=3.988, loss_v1=0, loss_v2=0, nll_loss=2.043, ntokens=381.3, nsentences=32, sample_size=381.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.771972, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.063, expert1_balance_bottom=16.974, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.12, wps=88.4, ups=0.23, wpb=381.3, bsz=32, num_updates=200, lr=9.41176e-07, gnorm=5.327, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=521
2022-05-16 20:35:28 - progress_bar.py[line:272] - INFO: epoch 001:    212 / 17711 loss=3.954, loss_v1=0, loss_v2=0, nll_loss=1.99, ntokens=381.9, nsentences=32, sample_size=381.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.785772, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.939, expert1_balance_bottom=17.195, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.97, wps=89.5, ups=0.23, wpb=381.9, bsz=32, num_updates=210, lr=9.88235e-07, gnorm=4.564, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=564
2022-05-16 20:36:11 - progress_bar.py[line:272] - INFO: epoch 001:    222 / 17711 loss=4.005, loss_v1=0, loss_v2=0, nll_loss=2.067, ntokens=376.1, nsentences=32, sample_size=376.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.769647, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.631, expert1_balance_bottom=17.456, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.19, wps=87.6, ups=0.23, wpb=376.1, bsz=32, num_updates=220, lr=1.03529e-06, gnorm=4.447, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=607
2022-05-16 20:36:54 - progress_bar.py[line:272] - INFO: epoch 001:    232 / 17711 loss=3.931, loss_v1=0, loss_v2=0, nll_loss=1.994, ntokens=383.9, nsentences=32, sample_size=383.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.761586, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.096, expert1_balance_bottom=16.655, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.98, wps=88.5, ups=0.23, wpb=383.9, bsz=32, num_updates=230, lr=1.08235e-06, gnorm=4.588, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=650
2022-05-16 20:37:39 - progress_bar.py[line:272] - INFO: epoch 001:    242 / 17711 loss=3.813, loss_v1=0, loss_v2=0, nll_loss=1.863, ntokens=386.4, nsentences=32, sample_size=386.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.761012, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.073, expert1_balance_bottom=16.558, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.64, wps=86.1, ups=0.22, wpb=386.4, bsz=32, num_updates=240, lr=1.12941e-06, gnorm=3.878, loss_scale=32, train_wall=19, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=695
2022-05-16 20:38:21 - progress_bar.py[line:272] - INFO: epoch 001:    252 / 17711 loss=3.958, loss_v1=0, loss_v2=0, nll_loss=2.013, ntokens=382.8, nsentences=32, sample_size=382.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.768347, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.291, expert1_balance_bottom=16.744, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.04, wps=91.4, ups=0.24, wpb=382.8, bsz=32, num_updates=250, lr=1.17647e-06, gnorm=4, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=737
2022-05-16 20:39:04 - progress_bar.py[line:272] - INFO: epoch 001:    262 / 17711 loss=4.031, loss_v1=0, loss_v2=0, nll_loss=2.108, ntokens=382.7, nsentences=32, sample_size=382.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.75931, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.163, expert1_balance_bottom=17.531, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.31, wps=89.9, ups=0.23, wpb=382.7, bsz=32, num_updates=260, lr=1.22353e-06, gnorm=4.516, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=780
2022-05-16 20:39:47 - progress_bar.py[line:272] - INFO: epoch 001:    272 / 17711 loss=4.079, loss_v1=0, loss_v2=0, nll_loss=2.132, ntokens=376.1, nsentences=32, sample_size=376.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.781375, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.748, expert1_balance_bottom=17.704, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.38, wps=85.7, ups=0.23, wpb=376.1, bsz=32, num_updates=270, lr=1.27059e-06, gnorm=4.359, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=823
2022-05-16 20:40:30 - progress_bar.py[line:272] - INFO: epoch 001:    282 / 17711 loss=3.929, loss_v1=0, loss_v2=0, nll_loss=1.997, ntokens=383.1, nsentences=32, sample_size=383.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.756167, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.824, expert1_balance_bottom=16.53, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.99, wps=89.6, ups=0.23, wpb=383.1, bsz=32, num_updates=280, lr=1.31765e-06, gnorm=4.388, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=866
2022-05-16 20:41:14 - progress_bar.py[line:272] - INFO: epoch 001:    292 / 17711 loss=4.078, loss_v1=0, loss_v2=0, nll_loss=2.127, ntokens=371.7, nsentences=32, sample_size=371.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.788881, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.504, expert1_balance_bottom=17.26, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.37, wps=85.3, ups=0.23, wpb=371.7, bsz=32, num_updates=290, lr=1.36471e-06, gnorm=4.66, loss_scale=32, train_wall=19, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=910
2022-05-16 20:41:57 - progress_bar.py[line:272] - INFO: epoch 001:    302 / 17711 loss=3.955, loss_v1=0, loss_v2=0, nll_loss=2.065, ntokens=386.4, nsentences=32, sample_size=386.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.722852, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=39.529, expert1_balance_bottom=16.027, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.18, wps=89.8, ups=0.23, wpb=386.4, bsz=32, num_updates=300, lr=1.41176e-06, gnorm=3.732, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=953
2022-05-16 20:42:40 - progress_bar.py[line:272] - INFO: epoch 001:    312 / 17711 loss=4.046, loss_v1=0, loss_v2=0, nll_loss=2.118, ntokens=383.4, nsentences=32, sample_size=383.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.766447, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.555, expert1_balance_bottom=16.539, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.34, wps=88, ups=0.23, wpb=383.4, bsz=32, num_updates=310, lr=1.45882e-06, gnorm=4.248, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=996
2022-05-16 20:43:23 - progress_bar.py[line:272] - INFO: epoch 001:    322 / 17711 loss=3.93, loss_v1=0, loss_v2=0, nll_loss=2.004, ntokens=388.7, nsentences=32, sample_size=388.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.749153, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.984, expert1_balance_bottom=16.099, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.01, wps=92.1, ups=0.24, wpb=388.7, bsz=32, num_updates=320, lr=1.50588e-06, gnorm=4.214, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1038
2022-05-16 20:44:06 - progress_bar.py[line:272] - INFO: epoch 001:    332 / 17711 loss=4.006, loss_v1=0, loss_v2=0, nll_loss=2.091, ntokens=378.8, nsentences=32, sample_size=378.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.749546, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.504, expert1_balance_bottom=16.506, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.26, wps=87, ups=0.23, wpb=378.8, bsz=32, num_updates=330, lr=1.55294e-06, gnorm=3.668, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1082
2022-05-16 20:44:48 - progress_bar.py[line:272] - INFO: epoch 001:    342 / 17711 loss=3.99, loss_v1=0, loss_v2=0, nll_loss=2.038, ntokens=386.1, nsentences=32, sample_size=386.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.780201, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.89, expert1_balance_bottom=16.987, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.11, wps=91.3, ups=0.24, wpb=386.1, bsz=32, num_updates=340, lr=1.6e-06, gnorm=3.896, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1124
2022-05-16 20:45:32 - progress_bar.py[line:272] - INFO: epoch 001:    352 / 17711 loss=3.9, loss_v1=0, loss_v2=0, nll_loss=1.935, ntokens=381, nsentences=32, sample_size=381, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.787334, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.036, expert1_balance_bottom=17.218, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.82, wps=87, ups=0.23, wpb=381, bsz=32, num_updates=350, lr=1.64706e-06, gnorm=3.629, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1168
2022-05-16 20:46:14 - progress_bar.py[line:272] - INFO: epoch 001:    362 / 17711 loss=3.877, loss_v1=0, loss_v2=0, nll_loss=1.924, ntokens=382.5, nsentences=32, sample_size=382.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.771811, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.307, expert1_balance_bottom=17.144, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.8, wps=90.6, ups=0.24, wpb=382.5, bsz=32, num_updates=360, lr=1.69412e-06, gnorm=3.469, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1210
2022-05-16 20:46:57 - progress_bar.py[line:272] - INFO: epoch 001:    372 / 17711 loss=3.94, loss_v1=0, loss_v2=0, nll_loss=1.994, ntokens=377.7, nsentences=32, sample_size=377.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.76936, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.923, expert1_balance_bottom=16.071, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.98, wps=89.4, ups=0.24, wpb=377.7, bsz=32, num_updates=370, lr=1.74118e-06, gnorm=3.682, loss_scale=32, train_wall=17, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1253
2022-05-16 20:47:37 - progress_bar.py[line:272] - INFO: epoch 001:    382 / 17711 loss=3.901, loss_v1=0, loss_v2=0, nll_loss=1.932, ntokens=379.3, nsentences=32, sample_size=379.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.782404, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.257, expert1_balance_bottom=16.951, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.82, wps=93.3, ups=0.25, wpb=379.3, bsz=32, num_updates=380, lr=1.78824e-06, gnorm=3.771, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1293
2022-05-16 20:48:19 - progress_bar.py[line:272] - INFO: epoch 001:    392 / 17711 loss=3.863, loss_v1=0, loss_v2=0, nll_loss=1.922, ntokens=385, nsentences=32, sample_size=385, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.756805, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.562, expert1_balance_bottom=16.322, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.79, wps=91.8, ups=0.24, wpb=385, bsz=32, num_updates=390, lr=1.83529e-06, gnorm=3.419, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1335
2022-05-16 20:49:02 - progress_bar.py[line:272] - INFO: epoch 001:    402 / 17711 loss=4.04, loss_v1=0, loss_v2=0, nll_loss=2.122, ntokens=383.5, nsentences=32, sample_size=383.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.748883, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.785, expert1_balance_bottom=16.45, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.35, wps=89.9, ups=0.23, wpb=383.5, bsz=32, num_updates=400, lr=1.88235e-06, gnorm=3.779, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1378
2022-05-16 20:49:44 - progress_bar.py[line:272] - INFO: epoch 001:    412 / 17711 loss=3.887, loss_v1=0, loss_v2=0, nll_loss=1.926, ntokens=383.4, nsentences=32, sample_size=383.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.779547, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.842, expert1_balance_bottom=16.838, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.8, wps=90.7, ups=0.24, wpb=383.4, bsz=32, num_updates=410, lr=1.92941e-06, gnorm=4.066, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1420
2022-05-16 20:50:27 - progress_bar.py[line:272] - INFO: epoch 001:    422 / 17711 loss=3.908, loss_v1=0, loss_v2=0, nll_loss=1.937, ntokens=377.8, nsentences=32, sample_size=377.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.787046, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.23, expert1_balance_bottom=17.642, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.83, wps=88.6, ups=0.23, wpb=377.8, bsz=32, num_updates=420, lr=1.97647e-06, gnorm=3.969, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1463
2022-05-16 20:51:10 - progress_bar.py[line:272] - INFO: epoch 001:    432 / 17711 loss=3.867, loss_v1=0, loss_v2=0, nll_loss=1.896, ntokens=381.2, nsentences=32, sample_size=381.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.782077, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.792, expert1_balance_bottom=16.67, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.72, wps=88.7, ups=0.23, wpb=381.2, bsz=32, num_updates=430, lr=2.02353e-06, gnorm=3.281, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1506
2022-05-16 20:51:52 - progress_bar.py[line:272] - INFO: epoch 001:    442 / 17711 loss=3.966, loss_v1=0, loss_v2=0, nll_loss=1.988, ntokens=381.6, nsentences=32, sample_size=381.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.797937, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.385, expert1_balance_top=35.713, expert1_balance_bottom=17.38, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.97, wps=91.2, ups=0.24, wpb=381.6, bsz=32, num_updates=440, lr=2.07059e-06, gnorm=3.449, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1548
2022-05-16 20:52:34 - progress_bar.py[line:272] - INFO: epoch 001:    452 / 17711 loss=3.942, loss_v1=0, loss_v2=0, nll_loss=1.977, ntokens=377.2, nsentences=32, sample_size=377.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.784361, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.751, expert1_balance_bottom=15.819, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.94, wps=89.8, ups=0.24, wpb=377.2, bsz=32, num_updates=450, lr=2.11765e-06, gnorm=3.648, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1590
2022-05-16 20:53:15 - progress_bar.py[line:272] - INFO: epoch 001:    462 / 17711 loss=3.831, loss_v1=0, loss_v2=0, nll_loss=1.854, ntokens=389.3, nsentences=32, sample_size=389.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.781725, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.946, expert1_balance_bottom=17.333, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.61, wps=95.1, ups=0.24, wpb=389.3, bsz=32, num_updates=460, lr=2.16471e-06, gnorm=3.566, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1631
2022-05-16 20:53:58 - progress_bar.py[line:272] - INFO: epoch 001:    472 / 17711 loss=3.998, loss_v1=0, loss_v2=0, nll_loss=2.018, ntokens=384.6, nsentences=32, sample_size=384.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.80356, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.385, expert1_balance_top=34.761, expert1_balance_bottom=18.08, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.05, wps=89.4, ups=0.23, wpb=384.6, bsz=32, num_updates=470, lr=2.21176e-06, gnorm=3.78, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1674
2022-05-16 20:54:40 - progress_bar.py[line:272] - INFO: epoch 001:    482 / 17711 loss=3.883, loss_v1=0, loss_v2=0, nll_loss=1.917, ntokens=384.4, nsentences=32, sample_size=384.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.777367, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.79, expert1_balance_bottom=16.795, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.78, wps=90.7, ups=0.24, wpb=384.4, bsz=32, num_updates=480, lr=2.25882e-06, gnorm=3.464, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1716
2022-05-16 20:55:22 - progress_bar.py[line:272] - INFO: epoch 001:    492 / 17711 loss=3.95, loss_v1=0, loss_v2=0, nll_loss=2.034, ntokens=398.2, nsentences=32, sample_size=398.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.733788, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=39.207, expert1_balance_bottom=15.283, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.09, wps=94.3, ups=0.24, wpb=398.2, bsz=32, num_updates=490, lr=2.30588e-06, gnorm=3.998, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1758
2022-05-16 20:56:04 - progress_bar.py[line:272] - INFO: epoch 001:    502 / 17711 loss=3.932, loss_v1=0, loss_v2=0, nll_loss=1.992, ntokens=381.1, nsentences=32, sample_size=381.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.758194, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.799, expert1_balance_bottom=16.743, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.98, wps=90.4, ups=0.24, wpb=381.1, bsz=32, num_updates=500, lr=2.35294e-06, gnorm=3.705, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1800
2022-05-16 20:56:47 - progress_bar.py[line:272] - INFO: epoch 001:    512 / 17711 loss=3.975, loss_v1=0, loss_v2=0, nll_loss=2.02, ntokens=380.4, nsentences=32, sample_size=380.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.776652, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.385, expert1_balance_top=36.273, expert1_balance_bottom=17.141, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.05, wps=90.1, ups=0.24, wpb=380.4, bsz=32, num_updates=510, lr=2.4e-06, gnorm=3.451, loss_scale=32, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1843
2022-05-16 20:57:29 - progress_bar.py[line:272] - INFO: epoch 001:    522 / 17711 loss=3.967, loss_v1=0, loss_v2=0, nll_loss=2.033, ntokens=375.7, nsentences=32, sample_size=375.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.755733, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.069, expert1_balance_bottom=16.646, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.09, wps=89.4, ups=0.24, wpb=375.7, bsz=32, num_updates=520, lr=2.44706e-06, gnorm=3.369, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1885
2022-05-16 20:58:11 - progress_bar.py[line:272] - INFO: epoch 001:    532 / 17711 loss=3.921, loss_v1=0, loss_v2=0, nll_loss=1.954, ntokens=380.5, nsentences=32, sample_size=380.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.782833, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.315, expert1_balance_bottom=16.836, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.87, wps=89.9, ups=0.24, wpb=380.5, bsz=32, num_updates=530, lr=2.49412e-06, gnorm=3.284, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1927
2022-05-16 20:58:54 - progress_bar.py[line:272] - INFO: epoch 001:    542 / 17711 loss=3.901, loss_v1=0, loss_v2=0, nll_loss=1.942, ntokens=383.4, nsentences=32, sample_size=383.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.775305, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.148, expert1_balance_bottom=17.273, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.84, wps=89.1, ups=0.23, wpb=383.4, bsz=32, num_updates=540, lr=2.54118e-06, gnorm=3.567, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=1970
2022-05-16 20:59:35 - progress_bar.py[line:272] - INFO: epoch 001:    552 / 17711 loss=4.017, loss_v1=0, loss_v2=0, nll_loss=2.062, ntokens=379.5, nsentences=32, sample_size=379.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.776869, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.663, expert1_balance_bottom=15.985, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.18, wps=92.2, ups=0.24, wpb=379.5, bsz=32, num_updates=550, lr=2.58824e-06, gnorm=3.577, loss_scale=64, train_wall=17, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2011
2022-05-16 21:00:17 - progress_bar.py[line:272] - INFO: epoch 001:    562 / 17711 loss=3.907, loss_v1=0, loss_v2=0, nll_loss=1.951, ntokens=380.5, nsentences=32, sample_size=380.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.765306, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.444, expert1_balance_bottom=16.44, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.87, wps=90.4, ups=0.24, wpb=380.5, bsz=32, num_updates=560, lr=2.63529e-06, gnorm=3.357, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2053
2022-05-16 21:00:59 - progress_bar.py[line:272] - INFO: epoch 001:    572 / 17711 loss=3.961, loss_v1=0, loss_v2=0, nll_loss=1.998, ntokens=379.5, nsentences=32, sample_size=379.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.779973, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.339, expert1_balance_bottom=17.112, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4, wps=90.2, ups=0.24, wpb=379.5, bsz=32, num_updates=570, lr=2.68235e-06, gnorm=3.461, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2095
2022-05-16 21:01:41 - progress_bar.py[line:272] - INFO: epoch 001:    582 / 17711 loss=3.922, loss_v1=0, loss_v2=0, nll_loss=1.968, ntokens=380.2, nsentences=32, sample_size=380.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.770198, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.73, expert1_balance_bottom=16.328, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.91, wps=91.9, ups=0.24, wpb=380.2, bsz=32, num_updates=580, lr=2.72941e-06, gnorm=3.484, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2137
2022-05-16 21:02:22 - progress_bar.py[line:272] - INFO: epoch 001:    592 / 17711 loss=3.848, loss_v1=0, loss_v2=0, nll_loss=1.87, ntokens=381.7, nsentences=32, sample_size=381.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.781491, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.867, expert1_balance_bottom=16.998, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.65, wps=92.8, ups=0.24, wpb=381.7, bsz=32, num_updates=590, lr=2.77647e-06, gnorm=3.973, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2178
2022-05-16 21:03:04 - progress_bar.py[line:272] - INFO: epoch 001:    602 / 17711 loss=4.005, loss_v1=0, loss_v2=0, nll_loss=2.017, ntokens=376.8, nsentences=32, sample_size=376.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.806414, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.385, expert1_balance_top=35.712, expert1_balance_bottom=17.976, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.05, wps=88.7, ups=0.24, wpb=376.8, bsz=32, num_updates=600, lr=2.82353e-06, gnorm=3.262, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2220
2022-05-16 21:03:46 - progress_bar.py[line:272] - INFO: epoch 001:    612 / 17711 loss=3.974, loss_v1=0, loss_v2=0, nll_loss=2.028, ntokens=388.9, nsentences=32, sample_size=388.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.764362, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.806, expert1_balance_bottom=15.942, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.08, wps=92.6, ups=0.24, wpb=388.9, bsz=32, num_updates=610, lr=2.87059e-06, gnorm=3.782, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2262
2022-05-16 21:04:28 - progress_bar.py[line:272] - INFO: epoch 001:    622 / 17711 loss=3.843, loss_v1=0, loss_v2=0, nll_loss=1.856, ntokens=385.5, nsentences=32, sample_size=385.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.788808, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.385, expert1_balance_top=35.614, expert1_balance_bottom=16.546, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.62, wps=92.9, ups=0.24, wpb=385.5, bsz=32, num_updates=620, lr=2.91765e-06, gnorm=3.748, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2304
2022-05-16 21:05:10 - progress_bar.py[line:272] - INFO: epoch 001:    632 / 17711 loss=3.881, loss_v1=0, loss_v2=0, nll_loss=1.913, ntokens=381.9, nsentences=32, sample_size=381.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.771549, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.385, expert1_balance_top=35.996, expert1_balance_bottom=17.227, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.77, wps=90.4, ups=0.24, wpb=381.9, bsz=32, num_updates=630, lr=2.96471e-06, gnorm=3.479, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2346
2022-05-16 21:05:53 - progress_bar.py[line:272] - INFO: epoch 001:    642 / 17711 loss=3.946, loss_v1=0, loss_v2=0, nll_loss=1.98, ntokens=385.2, nsentences=32, sample_size=385.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.78087, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.385, expert1_balance_top=36.558, expert1_balance_bottom=16.556, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.95, wps=90.6, ups=0.24, wpb=385.2, bsz=32, num_updates=640, lr=3.01176e-06, gnorm=3.719, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2388
2022-05-16 21:06:35 - progress_bar.py[line:272] - INFO: epoch 001:    652 / 17711 loss=3.904, loss_v1=0, loss_v2=0, nll_loss=1.951, ntokens=385.3, nsentences=32, sample_size=385.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.761903, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.471, expert1_balance_bottom=16.966, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.87, wps=91.3, ups=0.24, wpb=385.3, bsz=32, num_updates=650, lr=3.05882e-06, gnorm=3.328, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2431
2022-05-16 21:07:17 - progress_bar.py[line:272] - INFO: epoch 001:    662 / 17711 loss=3.887, loss_v1=0, loss_v2=0, nll_loss=1.926, ntokens=391.9, nsentences=32, sample_size=391.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.767974, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.415, expert1_balance_bottom=16.537, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.8, wps=93.1, ups=0.24, wpb=391.9, bsz=32, num_updates=660, lr=3.10588e-06, gnorm=3.247, loss_scale=64, train_wall=17, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2473
2022-05-16 21:08:00 - progress_bar.py[line:272] - INFO: epoch 001:    672 / 17711 loss=3.898, loss_v1=0, loss_v2=0, nll_loss=1.945, ntokens=374.1, nsentences=32, sample_size=374.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.76121, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.506, expert1_balance_bottom=16.564, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.85, wps=87.4, ups=0.23, wpb=374.1, bsz=32, num_updates=670, lr=3.15294e-06, gnorm=3.898, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2516
2022-05-16 21:08:42 - progress_bar.py[line:272] - INFO: epoch 001:    682 / 17711 loss=3.825, loss_v1=0, loss_v2=0, nll_loss=1.853, ntokens=386.2, nsentences=32, sample_size=386.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.770325, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.38, expert1_balance_bottom=17.047, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.61, wps=90.8, ups=0.24, wpb=386.2, bsz=32, num_updates=680, lr=3.2e-06, gnorm=3.371, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2558
2022-05-16 21:09:23 - progress_bar.py[line:272] - INFO: epoch 001:    692 / 17711 loss=3.918, loss_v1=0, loss_v2=0, nll_loss=1.959, ntokens=384.2, nsentences=32, sample_size=384.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.77068, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.592, expert1_balance_bottom=16.983, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.89, wps=94.9, ups=0.25, wpb=384.2, bsz=32, num_updates=690, lr=3.24706e-06, gnorm=3.166, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2599
2022-05-16 21:10:06 - progress_bar.py[line:272] - INFO: epoch 001:    702 / 17711 loss=3.969, loss_v1=0, loss_v2=0, nll_loss=2.017, ntokens=393.1, nsentences=32, sample_size=393.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.765504, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.765, expert1_balance_bottom=16.288, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.05, wps=91.8, ups=0.23, wpb=393.1, bsz=32, num_updates=700, lr=3.29412e-06, gnorm=3.402, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2641
2022-05-16 21:10:48 - progress_bar.py[line:272] - INFO: epoch 001:    712 / 17711 loss=3.907, loss_v1=0, loss_v2=0, nll_loss=1.962, ntokens=386.6, nsentences=32, sample_size=386.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.752372, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.007, expert1_balance_bottom=15.365, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.9, wps=91.3, ups=0.24, wpb=386.6, bsz=32, num_updates=710, lr=3.34118e-06, gnorm=3.543, loss_scale=64, train_wall=17, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2684
2022-05-16 21:11:29 - progress_bar.py[line:272] - INFO: epoch 001:    722 / 17711 loss=3.942, loss_v1=0, loss_v2=0, nll_loss=1.992, ntokens=391.4, nsentences=32, sample_size=391.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.763658, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.925, expert1_balance_bottom=16.25, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.98, wps=94.7, ups=0.24, wpb=391.4, bsz=32, num_updates=720, lr=3.38824e-06, gnorm=3.36, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2725
2022-05-16 21:12:11 - progress_bar.py[line:272] - INFO: epoch 001:    732 / 17711 loss=3.862, loss_v1=0, loss_v2=0, nll_loss=1.908, ntokens=383.8, nsentences=32, sample_size=383.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.756339, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.299, expert1_balance_bottom=15.936, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.75, wps=90.7, ups=0.24, wpb=383.8, bsz=32, num_updates=730, lr=3.43529e-06, gnorm=3.541, loss_scale=64, train_wall=19, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2767
2022-05-16 21:12:54 - progress_bar.py[line:272] - INFO: epoch 001:    742 / 17711 loss=3.956, loss_v1=0, loss_v2=0, nll_loss=2.048, ntokens=384, nsentences=32, sample_size=384, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.722802, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=39.251, expert1_balance_bottom=15.689, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.13, wps=91.1, ups=0.24, wpb=384, bsz=32, num_updates=740, lr=3.48235e-06, gnorm=4.081, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2810
2022-05-16 21:13:35 - progress_bar.py[line:272] - INFO: epoch 001:    752 / 17711 loss=3.872, loss_v1=0, loss_v2=0, nll_loss=1.906, ntokens=386.4, nsentences=32, sample_size=386.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.769021, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.935, expert1_balance_bottom=17.396, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.75, wps=93.7, ups=0.24, wpb=386.4, bsz=32, num_updates=750, lr=3.52941e-06, gnorm=3.859, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2851
2022-05-16 21:14:17 - progress_bar.py[line:272] - INFO: epoch 001:    762 / 17711 loss=3.93, loss_v1=0, loss_v2=0, nll_loss=1.97, ntokens=379, nsentences=32, sample_size=379, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.770417, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.512, expert1_balance_bottom=16.941, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.92, wps=89, ups=0.23, wpb=379, bsz=32, num_updates=760, lr=3.57647e-06, gnorm=3.73, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2893
2022-05-16 21:15:00 - progress_bar.py[line:272] - INFO: epoch 001:    772 / 17711 loss=3.925, loss_v1=0, loss_v2=0, nll_loss=1.96, ntokens=384.8, nsentences=32, sample_size=384.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.770963, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.599, expert1_balance_bottom=16.156, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.89, wps=90.4, ups=0.23, wpb=384.8, bsz=32, num_updates=770, lr=3.62353e-06, gnorm=3.407, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2936
2022-05-16 21:15:43 - progress_bar.py[line:272] - INFO: epoch 001:    782 / 17711 loss=3.972, loss_v1=0, loss_v2=0, nll_loss=2.006, ntokens=377.1, nsentences=32, sample_size=377.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.778901, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.128, expert1_balance_bottom=17.166, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.02, wps=88.6, ups=0.23, wpb=377.1, bsz=32, num_updates=780, lr=3.67059e-06, gnorm=3.227, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=2978
2022-05-16 21:16:25 - progress_bar.py[line:272] - INFO: epoch 001:    792 / 17711 loss=3.903, loss_v1=0, loss_v2=0, nll_loss=1.972, ntokens=386.7, nsentences=32, sample_size=386.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.741964, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=40.516, expert1_balance_bottom=15.57, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.92, wps=90.6, ups=0.23, wpb=386.7, bsz=32, num_updates=790, lr=3.71765e-06, gnorm=3.856, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3021
2022-05-16 21:17:08 - progress_bar.py[line:272] - INFO: epoch 001:    802 / 17711 loss=3.957, loss_v1=0, loss_v2=0, nll_loss=2.01, ntokens=380.7, nsentences=32, sample_size=380.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.760873, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.422, expert1_balance_bottom=15.899, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.03, wps=90.1, ups=0.24, wpb=380.7, bsz=32, num_updates=800, lr=3.76471e-06, gnorm=3.677, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3063
2022-05-16 21:17:49 - progress_bar.py[line:272] - INFO: epoch 001:    812 / 17711 loss=3.945, loss_v1=0, loss_v2=0, nll_loss=1.975, ntokens=384.3, nsentences=32, sample_size=384.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.781487, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.603, expert1_balance_bottom=15.936, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.93, wps=92.4, ups=0.24, wpb=384.3, bsz=32, num_updates=810, lr=3.81176e-06, gnorm=3.165, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3105
2022-05-16 21:18:32 - progress_bar.py[line:272] - INFO: epoch 001:    822 / 17711 loss=3.928, loss_v1=0, loss_v2=0, nll_loss=1.987, ntokens=393.1, nsentences=32, sample_size=393.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.752386, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.57, expert1_balance_bottom=16.346, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.97, wps=92.5, ups=0.24, wpb=393.1, bsz=32, num_updates=820, lr=3.85882e-06, gnorm=3.151, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3148
2022-05-16 21:19:13 - progress_bar.py[line:272] - INFO: epoch 001:    832 / 17711 loss=3.843, loss_v1=0, loss_v2=0, nll_loss=1.866, ntokens=384.2, nsentences=32, sample_size=384.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.776447, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.23, expert1_balance_bottom=16.467, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.64, wps=92.7, ups=0.24, wpb=384.2, bsz=32, num_updates=830, lr=3.90588e-06, gnorm=3.347, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3189
2022-05-16 21:19:56 - progress_bar.py[line:272] - INFO: epoch 001:    842 / 17711 loss=3.819, loss_v1=0, loss_v2=0, nll_loss=1.851, ntokens=387, nsentences=32, sample_size=387, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.760659, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.975, expert1_balance_bottom=16.399, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.61, wps=91.2, ups=0.24, wpb=387, bsz=32, num_updates=840, lr=3.95294e-06, gnorm=2.918, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3231
2022-05-16 21:20:38 - progress_bar.py[line:272] - INFO: epoch 001:    852 / 17711 loss=3.911, loss_v1=0, loss_v2=0, nll_loss=1.986, ntokens=393.4, nsentences=32, sample_size=393.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.733366, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=40.496, expert1_balance_bottom=15.749, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.96, wps=93, ups=0.24, wpb=393.4, bsz=32, num_updates=850, lr=4e-06, gnorm=3.597, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3274
2022-05-16 21:21:21 - progress_bar.py[line:272] - INFO: epoch 001:    862 / 17711 loss=3.827, loss_v1=0, loss_v2=0, nll_loss=1.856, ntokens=381.4, nsentences=32, sample_size=381.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.766397, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.67, expert1_balance_bottom=16.555, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.62, wps=89.1, ups=0.23, wpb=381.4, bsz=32, num_updates=860, lr=4.04706e-06, gnorm=3.271, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3316
2022-05-16 21:22:03 - progress_bar.py[line:272] - INFO: epoch 001:    872 / 17711 loss=3.906, loss_v1=0, loss_v2=0, nll_loss=1.938, ntokens=382.6, nsentences=32, sample_size=382.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.769974, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.88, expert1_balance_bottom=15.755, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.83, wps=90.6, ups=0.24, wpb=382.6, bsz=32, num_updates=870, lr=4.09412e-06, gnorm=3.737, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3359
2022-05-16 21:22:45 - progress_bar.py[line:272] - INFO: epoch 001:    882 / 17711 loss=3.948, loss_v1=0, loss_v2=0, nll_loss=1.99, ntokens=380.5, nsentences=32, sample_size=380.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.768299, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.157, expert1_balance_bottom=15.632, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.97, wps=90.7, ups=0.24, wpb=380.5, bsz=32, num_updates=880, lr=4.14118e-06, gnorm=3.311, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3401
2022-05-16 21:23:27 - progress_bar.py[line:272] - INFO: epoch 001:    892 / 17711 loss=3.926, loss_v1=0, loss_v2=0, nll_loss=1.984, ntokens=389, nsentences=32, sample_size=389, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.75188, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.49, expert1_balance_bottom=15.38, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.96, wps=92.8, ups=0.24, wpb=389, bsz=32, num_updates=890, lr=4.18824e-06, gnorm=4.187, loss_scale=64, train_wall=17, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3443
2022-05-16 21:24:08 - progress_bar.py[line:272] - INFO: epoch 001:    902 / 17711 loss=3.885, loss_v1=0, loss_v2=0, nll_loss=1.912, ntokens=386.4, nsentences=32, sample_size=386.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.778019, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.372, expert1_balance_bottom=16.965, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.76, wps=93.2, ups=0.24, wpb=386.4, bsz=32, num_updates=900, lr=4.23529e-06, gnorm=3.464, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3484
2022-05-16 21:24:51 - progress_bar.py[line:272] - INFO: epoch 001:    912 / 17711 loss=3.996, loss_v1=0, loss_v2=0, nll_loss=2.055, ntokens=387.2, nsentences=32, sample_size=387.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.755246, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.98, expert1_balance_bottom=15.344, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.15, wps=90.5, ups=0.23, wpb=387.2, bsz=32, num_updates=910, lr=4.28235e-06, gnorm=3.644, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3527
2022-05-16 21:25:33 - progress_bar.py[line:272] - INFO: epoch 001:    922 / 17711 loss=3.881, loss_v1=0, loss_v2=0, nll_loss=1.905, ntokens=374.2, nsentences=32, sample_size=374.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.775258, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.53, expert1_balance_bottom=16.527, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.75, wps=89.6, ups=0.24, wpb=374.2, bsz=32, num_updates=920, lr=4.32941e-06, gnorm=3.356, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3569
2022-05-16 21:26:14 - progress_bar.py[line:272] - INFO: epoch 001:    932 / 17711 loss=3.879, loss_v1=0, loss_v2=0, nll_loss=1.907, ntokens=381, nsentences=32, sample_size=381, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.771525, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.529, expert1_balance_bottom=15.816, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.75, wps=91.2, ups=0.24, wpb=381, bsz=32, num_updates=930, lr=4.37647e-06, gnorm=3.317, loss_scale=64, train_wall=19, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3610
2022-05-16 21:26:57 - progress_bar.py[line:272] - INFO: epoch 001:    942 / 17711 loss=3.866, loss_v1=0, loss_v2=0, nll_loss=1.883, ntokens=377.1, nsentences=32, sample_size=377.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.778303, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.83, expert1_balance_bottom=16.144, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.69, wps=88.4, ups=0.23, wpb=377.1, bsz=32, num_updates=940, lr=4.42353e-06, gnorm=3.211, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3653
2022-05-16 21:27:40 - progress_bar.py[line:272] - INFO: epoch 001:    952 / 17711 loss=3.946, loss_v1=0, loss_v2=0, nll_loss=1.982, ntokens=378.4, nsentences=32, sample_size=378.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.771207, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.385, expert1_balance_top=37.849, expert1_balance_bottom=16.914, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.95, wps=89, ups=0.24, wpb=378.4, bsz=32, num_updates=950, lr=4.47059e-06, gnorm=3.688, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3696
2022-05-16 21:28:23 - progress_bar.py[line:272] - INFO: epoch 001:    962 / 17711 loss=3.89, loss_v1=0, loss_v2=0, nll_loss=1.934, ntokens=380.9, nsentences=32, sample_size=380.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.757432, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.224, expert1_balance_bottom=16.797, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.82, wps=88.4, ups=0.23, wpb=380.9, bsz=32, num_updates=960, lr=4.51765e-06, gnorm=3.353, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3739
2022-05-16 21:29:04 - progress_bar.py[line:272] - INFO: epoch 001:    972 / 17711 loss=3.805, loss_v1=0, loss_v2=0, nll_loss=1.845, ntokens=400.4, nsentences=32, sample_size=400.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.754258, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.805, expert1_balance_bottom=16.193, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.59, wps=95.7, ups=0.24, wpb=400.4, bsz=32, num_updates=970, lr=4.56471e-06, gnorm=3.352, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3780
2022-05-16 21:29:47 - progress_bar.py[line:272] - INFO: epoch 001:    982 / 17711 loss=3.856, loss_v1=0, loss_v2=0, nll_loss=1.871, ntokens=380.3, nsentences=32, sample_size=380.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.781671, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.381, expert1_balance_bottom=16.811, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.66, wps=88.9, ups=0.23, wpb=380.3, bsz=32, num_updates=980, lr=4.61176e-06, gnorm=3.493, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3823
2022-05-16 21:30:30 - progress_bar.py[line:272] - INFO: epoch 001:    992 / 17711 loss=3.837, loss_v1=0, loss_v2=0, nll_loss=1.876, ntokens=390.7, nsentences=32, sample_size=390.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.756055, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.623, expert1_balance_bottom=16.29, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.67, wps=91.9, ups=0.24, wpb=390.7, bsz=32, num_updates=990, lr=4.65882e-06, gnorm=3.208, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3866
2022-05-16 21:31:13 - progress_bar.py[line:272] - INFO: epoch 001:   1002 / 17711 loss=3.935, loss_v1=0, loss_v2=0, nll_loss=1.969, ntokens=379.8, nsentences=32, sample_size=379.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.768433, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.799, expert1_balance_bottom=16.158, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.92, wps=88.8, ups=0.23, wpb=379.8, bsz=32, num_updates=1000, lr=4.70588e-06, gnorm=3.162, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3908
2022-05-16 21:31:56 - progress_bar.py[line:272] - INFO: epoch 001:   1012 / 17711 loss=3.944, loss_v1=0, loss_v2=0, nll_loss=1.969, ntokens=377.8, nsentences=32, sample_size=377.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.782638, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.462, expert1_balance_bottom=15.471, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.92, wps=87.4, ups=0.23, wpb=377.8, bsz=32, num_updates=1010, lr=4.75294e-06, gnorm=3.876, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3952
2022-05-16 21:32:38 - progress_bar.py[line:272] - INFO: epoch 001:   1022 / 17711 loss=3.907, loss_v1=0, loss_v2=0, nll_loss=1.912, ntokens=373.2, nsentences=32, sample_size=373.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.791475, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.079, expert1_balance_bottom=16.448, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.76, wps=88.4, ups=0.24, wpb=373.2, bsz=32, num_updates=1020, lr=4.8e-06, gnorm=3.519, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=3994
2022-05-16 21:33:21 - progress_bar.py[line:272] - INFO: epoch 001:   1032 / 17711 loss=3.893, loss_v1=0, loss_v2=0, nll_loss=1.91, ntokens=382.2, nsentences=32, sample_size=382.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.780648, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.385, expert1_balance_top=36.934, expert1_balance_bottom=16.546, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.76, wps=88.8, ups=0.23, wpb=382.2, bsz=32, num_updates=1030, lr=4.84706e-06, gnorm=3.386, loss_scale=128, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4037
2022-05-16 21:34:04 - progress_bar.py[line:272] - INFO: epoch 001:   1042 / 17711 loss=3.874, loss_v1=0, loss_v2=0, nll_loss=1.912, ntokens=375.2, nsentences=32, sample_size=375.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.762789, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.139, expert1_balance_bottom=16.718, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.76, wps=86.9, ups=0.23, wpb=375.2, bsz=32, num_updates=1040, lr=4.89412e-06, gnorm=3.461, loss_scale=128, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4080
2022-05-16 21:34:47 - progress_bar.py[line:272] - INFO: epoch 001:   1052 / 17711 loss=3.927, loss_v1=0, loss_v2=0, nll_loss=1.945, ntokens=383.7, nsentences=32, sample_size=383.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.781153, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=39.125, expert1_balance_bottom=15.811, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.85, wps=90.2, ups=0.24, wpb=383.7, bsz=32, num_updates=1050, lr=4.94118e-06, gnorm=3.291, loss_scale=128, train_wall=19, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4123
2022-05-16 21:35:30 - progress_bar.py[line:272] - INFO: epoch 001:   1062 / 17711 loss=3.924, loss_v1=0, loss_v2=0, nll_loss=1.967, ntokens=394, nsentences=32, sample_size=394, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.758153, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=39.04, expert1_balance_bottom=15.988, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.91, wps=91.5, ups=0.23, wpb=394, bsz=32, num_updates=1060, lr=4.98824e-06, gnorm=3.847, loss_scale=128, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4166
2022-05-16 21:36:12 - progress_bar.py[line:272] - INFO: epoch 001:   1072 / 17711 loss=3.986, loss_v1=0, loss_v2=0, nll_loss=2.039, ntokens=374.1, nsentences=32, sample_size=374.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.763995, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.04, expert1_balance_bottom=15.493, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.11, wps=88.3, ups=0.24, wpb=374.1, bsz=32, num_updates=1070, lr=5.03529e-06, gnorm=3.312, loss_scale=128, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4208
2022-05-16 21:36:54 - progress_bar.py[line:272] - INFO: epoch 001:   1082 / 17711 loss=3.887, loss_v1=0, loss_v2=0, nll_loss=1.932, ntokens=380.1, nsentences=32, sample_size=380.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.757539, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.795, expert1_balance_bottom=16.33, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.82, wps=91, ups=0.24, wpb=380.1, bsz=32, num_updates=1080, lr=5.08235e-06, gnorm=3.638, loss_scale=128, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4250
2022-05-16 21:37:36 - progress_bar.py[line:272] - INFO: epoch 001:   1092 / 17711 loss=3.907, loss_v1=0, loss_v2=0, nll_loss=1.925, ntokens=385.9, nsentences=32, sample_size=385.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.78777, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=35.795, expert1_balance_bottom=16.399, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.8, wps=91.5, ups=0.24, wpb=385.9, bsz=32, num_updates=1090, lr=5.12941e-06, gnorm=3.842, loss_scale=128, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4292
2022-05-16 21:38:18 - progress_bar.py[line:272] - INFO: epoch 001:   1102 / 17711 loss=3.857, loss_v1=0, loss_v2=0, nll_loss=1.888, ntokens=384.2, nsentences=32, sample_size=384.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.76879, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.8, expert1_balance_bottom=17.229, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.7, wps=92.1, ups=0.24, wpb=384.2, bsz=32, num_updates=1100, lr=5.17647e-06, gnorm=3.43, loss_scale=128, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4334
2022-05-16 21:39:00 - progress_bar.py[line:272] - INFO: epoch 001:   1112 / 17711 loss=3.919, loss_v1=0, loss_v2=0, nll_loss=1.945, ntokens=379.4, nsentences=32, sample_size=379.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.775251, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.462, expert1_balance_bottom=16.715, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.85, wps=88.9, ups=0.23, wpb=379.4, bsz=32, num_updates=1110, lr=5.22353e-06, gnorm=3.95, loss_scale=128, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4376
2022-05-16 21:39:43 - progress_bar.py[line:272] - INFO: epoch 001:   1122 / 17711 loss=3.847, loss_v1=0, loss_v2=0, nll_loss=1.875, ntokens=379.7, nsentences=32, sample_size=379.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.763495, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.487, expert1_balance_bottom=16.079, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.67, wps=89.9, ups=0.24, wpb=379.7, bsz=32, num_updates=1120, lr=5.27059e-06, gnorm=3.652, loss_scale=128, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4419
2022-05-16 21:39:51 - trainer.py[line:945] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-05-16 21:40:29 - progress_bar.py[line:272] - INFO: epoch 001:   1133 / 17711 loss=3.858, loss_v1=0, loss_v2=0, nll_loss=1.896, ntokens=381.8, nsentences=32, sample_size=381.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.761619, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.628, expert1_balance_bottom=16.637, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.72, wps=82.3, ups=0.22, wpb=381.8, bsz=32, num_updates=1130, lr=5.31765e-06, gnorm=3.507, loss_scale=64, train_wall=20, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4465
2022-05-16 21:41:11 - progress_bar.py[line:272] - INFO: epoch 001:   1143 / 17711 loss=3.856, loss_v1=0, loss_v2=0, nll_loss=1.891, ntokens=391.3, nsentences=32, sample_size=391.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.762051, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=39.132, expert1_balance_bottom=15.774, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.71, wps=92.4, ups=0.24, wpb=391.3, bsz=32, num_updates=1140, lr=5.36471e-06, gnorm=4.197, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4507
2022-05-16 21:41:55 - progress_bar.py[line:272] - INFO: epoch 001:   1153 / 17711 loss=3.912, loss_v1=0, loss_v2=0, nll_loss=1.953, ntokens=388.6, nsentences=32, sample_size=388.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.757305, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=38.062, expert1_balance_bottom=16.051, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.87, wps=89.8, ups=0.23, wpb=388.6, bsz=32, num_updates=1150, lr=5.41176e-06, gnorm=4.355, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4551
2022-05-16 21:42:36 - progress_bar.py[line:272] - INFO: epoch 001:   1163 / 17711 loss=3.838, loss_v1=0, loss_v2=0, nll_loss=1.859, ntokens=383.3, nsentences=32, sample_size=383.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.77162, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=37.97, expert1_balance_bottom=16.211, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.63, wps=92.2, ups=0.24, wpb=383.3, bsz=32, num_updates=1160, lr=5.45882e-06, gnorm=3.866, loss_scale=64, train_wall=18, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4592
2022-05-16 21:43:19 - progress_bar.py[line:272] - INFO: epoch 001:   1173 / 17711 loss=3.808, loss_v1=0, loss_v2=0, nll_loss=1.827, ntokens=387.9, nsentences=32, sample_size=387.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.769903, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.384, expert1_balance_top=36.68, expert1_balance_bottom=16.557, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.55, wps=90.8, ups=0.23, wpb=387.9, bsz=32, num_updates=1170, lr=5.50588e-06, gnorm=3.131, loss_scale=64, train_wall=17, cuda_gb_allocated=11.2, cuda_gb_reserved=11.7, cuda_gb_free=12.5, wall=4635
Killing subprocess 42265
Main process received SIGINT, exiting
