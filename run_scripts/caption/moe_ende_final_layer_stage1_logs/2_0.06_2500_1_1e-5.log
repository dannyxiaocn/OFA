2022-05-16 20:29:51 - utils.py[line:263] - INFO: distributed init (rank 0): env://
2022-05-16 20:29:51 - utils.py[line:269] - INFO: Start init
2022-05-16 20:29:51 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2022-05-16 20:29:51 - utils.py[line:279] - INFO: initialized host heming-ng1 as rank 0
single-machine distributed training is initialized.
2022-05-16 20:29:53 - train.py[line:77] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'log_nvidia_smi': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'is_moe': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'num_workers_valid': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 4000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './moe_ende_final_layer_stage1_checkpoints/2_0.06_2500_1_1e-5', 'restore_file': './moe_ende_final_layer_stage1_checkpoints/6_0.06_2500_1_1e-5/checkpoint.best_cider_1.2790.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 4000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_best_checkpoints': False, 'no_save_optimizer_state': False, 'no_save_optimizer_state_on_training_finished': False, 'symlink_best_and_last_checkpoints': False, 'best_checkpoint_metric': 'cider', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 's3_upload_path': None, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807, 'stats_path': None, 'max_valid_steps': None}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_moe_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_type_embedding=True, all_gather_list_size=16384, alternate_ffn_embed_dim=0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='ofa_moe_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=8, batch_size_valid=8, best_checkpoint_metric='cider', bf16=False, block_wise=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_moe_cross_entropy', cross_self_attention=False, curriculum=0, data='../../dataset/caption_data/caption_stage1_train.tsv,../../dataset/caption_data/caption_val.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_moe_freq=6, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, drop_worst_after=2500, drop_worst_ratio=0.2, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_moe_freq=6, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":1,"max_len_b":16,"no_repeat_ngram_size":3}', eval_bleu=False, eval_cider=True, eval_cider_cached_tokens='../../dataset/caption_data/cider_cached_tokens/coco-valid-words.p', eval_print_samples=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, log_nvidia_smi=False, lr=[1e-05], lr_scheduler='polynomial_decay', max_epoch=2, max_source_positions=1024, max_src_length=80, max_target_positions=1024, max_tgt_length=20, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, moe_eval_capacity_token_fraction=0.25, moe_expert_count=4, moe_expert_ffn_dim=0, moe_freq=6, moe_gate_loss_combine_method='average', moe_gate_loss_transform='none', moe_gate_loss_wt=1.0, moe_gating_use_fp32=True, moe_normalize_expert_grad='', moe_normalize_gate_prob_before_dropping=True, moe_second_expert_policy='all', moe_top1_expert=True, no_best_checkpoints=False, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_save_optimizer_state_on_training_finished=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_bins=1000, num_shards=1, num_workers=0, num_workers_valid=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='./moe_ende_final_layer_stage1_checkpoints/6_0.06_2500_1_1e-5/checkpoint.best_cider_1.2790.pt', s3_upload_path=None, sample_patch_num=196, save_dir='./moe_ende_final_layer_stage1_checkpoints/2_0.06_2500_1_1e-5', save_interval=1, save_interval_updates=4000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', scst=False, scst_args='{}', seed=1, selected_cols='0,4,2', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, symlink_best_and_last_checkpoints=False, sync_bn=False, task='caption', tensorboard_logdir=None, threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[4], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_moe_pad_mask=True, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=4000, wandb_project=None, warmup_ratio=0.06, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'caption', 'data': '../../dataset/caption_data/caption_stage1_train.tsv,../../dataset/caption_data/caption_val.tsv', 'selected_cols': '0,4,2', 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 80, 'max_tgt_length': 20, 'code_dict_size': 8192, 'patch_image_size': 480, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'eval_bleu': False, 'eval_cider': True, 'eval_args': '{"beam":1,"max_len_b":16,"no_repeat_ngram_size":3}', 'eval_print_samples': False, 'eval_cider_cached_tokens': '../../dataset/caption_data/cider_cached_tokens/coco-valid-words.p', 'scst': False, 'scst_args': '{}'}, 'criterion': {'_name': 'adjust_label_smoothed_moe_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.2, 'drop_worst_after': 2500, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None, 'moe_gate_loss_wt': 1.0, 'moe_gate_loss_combine_method': 'average', 'moe_gate_loss_transform': 'none'}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05], 'block_wise': False}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.06, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-05-16 20:29:53 - ofa_task.py[line:103] - INFO: source dictionary: 59457 types
2022-05-16 20:29:53 - ofa_task.py[line:104] - INFO: target dictionary: 59457 types
2022-05-16 20:29:56 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2022-05-16 20:29:56 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:3 to store for rank: 0
2022-05-16 20:29:56 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:4 to store for rank: 0
2022-05-16 20:30:02 - train.py[line:108] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (moe_layer): MOELayer(
          (gate): Top1Gate(
            (wg): Linear(in_features=768, out_features=4, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
          )
        )
        (attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): FusedLayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)
        (moe_layer): MOELayer(
          (gate): Top1Gate(
            (wg): Linear(in_features=768, out_features=4, bias=False)
          )
          (experts): ModuleList(
            (0): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (1): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (2): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
            (3): FeedForwardNetwork(
              (activation_dropout_module): FairseqDropout()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout_module): FairseqDropout()
            )
          )
        )
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2022-05-16 20:30:02 - train.py[line:109] - INFO: task: CaptionTask
2022-05-16 20:30:02 - train.py[line:110] - INFO: model: OFAModel
2022-05-16 20:30:02 - train.py[line:111] - INFO: criterion: AdjustLabelSmoothedMOECrossEntropyCriterion
2022-05-16 20:30:02 - train.py[line:112] - INFO: num. shared model params: 172,799,816 (num. trained: 127,136,840)
2022-05-16 20:30:02 - train.py[line:119] - INFO: num. expert model params: 37779456 (num. trained: 37779456)
local datafile ../../dataset/caption_data/caption_val.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_val.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_val.tsv slice_id 0 row count 5000 total row count 5000
/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.layers.5.moe_layer.gate.wg.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.layers.5.moe_layer.gate.wg.bias
2022-05-16 20:30:05 - trainer.py[line:129] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2022-05-16 20:30:05 - utils.py[line:779] - INFO: ***********************CUDA enviroments for all 1 workers***********************
2022-05-16 20:30:05 - utils.py[line:781] - INFO: rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-05-16 20:30:05 - utils.py[line:787] - INFO: ***********************CUDA enviroments for all 1 workers***********************
2022-05-16 20:30:05 - train.py[line:150] - INFO: training on 1 devices (GPUs/TPUs)
2022-05-16 20:30:05 - train.py[line:155] - INFO: max tokens per device = None and max sentences per device = 8
2022-05-16 20:30:05 - trainer.py[line:477] - INFO: Preparing to load checkpoint ./moe_ende_final_layer_stage1_checkpoints/6_0.06_2500_1_1e-5/checkpoint.best_cider_1.2790.pt
2022-05-16 20:30:13 - adam.py[line:70] - INFO: using FusedAdam
2022-05-16 20:30:13 - trainer.py[line:636] - INFO: Loaded checkpoint ./moe_ende_final_layer_stage1_checkpoints/6_0.06_2500_1_1e-5/checkpoint.best_cider_1.2790.pt (epoch 1 @ 0 updates)
2022-05-16 20:30:13 - trainer.py[line:658] - INFO: loading train data for epoch 1
local datafile ../../dataset/caption_data/caption_stage1_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/caption_data/caption_stage1_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/caption_data/caption_stage1_train.tsv slice_id 0 row count 566747 total row count 566747
slice_id 0 seek offset 0
Total steps 35422, warmup steps 2125, warmup_factor 0.00047058823529411766
2022-05-16 20:30:44 - trainer.py[line:722] - INFO: begin training epoch 1
2022-05-16 20:30:44 - train.py[line:303] - INFO: Start iterating over samples
2022-05-16 20:31:00 - trainer.py[line:945] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-05-16 20:31:13 - trainer.py[line:945] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-05-16 20:31:47 - progress_bar.py[line:272] - INFO: epoch 001:     12 / 17711 loss=4.105, loss_v1=0, loss_v2=0, nll_loss=2, ntokens=381.8, nsentences=32, sample_size=381.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.8887, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.911, expert1_balance_bottom=18.248, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4, wps=80.5, ups=0.21, wpb=381.8, bsz=32, num_updates=10, lr=4.70588e-08, gnorm=6.083, loss_scale=32, train_wall=36, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=102
2022-05-16 20:32:30 - progress_bar.py[line:272] - INFO: epoch 001:     22 / 17711 loss=4.126, loss_v1=0, loss_v2=0, nll_loss=2.034, ntokens=378.5, nsentences=32, sample_size=378.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.879759, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.293, expert1_balance_bottom=18.157, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.1, wps=88.8, ups=0.23, wpb=378.5, bsz=32, num_updates=20, lr=9.41176e-08, gnorm=6.131, loss_scale=32, train_wall=19, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=144
2022-05-16 20:33:12 - progress_bar.py[line:272] - INFO: epoch 001:     32 / 17711 loss=4.285, loss_v1=0, loss_v2=0, nll_loss=2.206, ntokens=379.3, nsentences=32, sample_size=379.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.883654, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.196, expert1_balance_bottom=18.299, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.61, wps=89.1, ups=0.23, wpb=379.3, bsz=32, num_updates=30, lr=1.41176e-07, gnorm=7.035, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=187
2022-05-16 20:33:55 - progress_bar.py[line:272] - INFO: epoch 001:     42 / 17711 loss=4.208, loss_v1=0, loss_v2=0, nll_loss=2.12, ntokens=372.6, nsentences=32, sample_size=372.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.884526, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.457, expert1_balance_bottom=18.315, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.35, wps=87, ups=0.23, wpb=372.6, bsz=32, num_updates=40, lr=1.88235e-07, gnorm=6.833, loss_scale=32, train_wall=19, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=230
2022-05-16 20:34:36 - progress_bar.py[line:272] - INFO: epoch 001:     52 / 17711 loss=4.171, loss_v1=0, loss_v2=0, nll_loss=2.086, ntokens=378.9, nsentences=32, sample_size=378.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.877929, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=37.279, expert1_balance_bottom=17.74, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.25, wps=91.7, ups=0.24, wpb=378.9, bsz=32, num_updates=50, lr=2.35294e-07, gnorm=6.573, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=271
2022-05-16 20:35:18 - progress_bar.py[line:272] - INFO: epoch 001:     62 / 17711 loss=4.08, loss_v1=0, loss_v2=0, nll_loss=1.97, ntokens=390, nsentences=32, sample_size=390, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.893834, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.131, expert1_balance_bottom=18.353, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.92, wps=94.5, ups=0.24, wpb=390, bsz=32, num_updates=60, lr=2.82353e-07, gnorm=6.302, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=312
2022-05-16 20:35:58 - progress_bar.py[line:272] - INFO: epoch 001:     72 / 17711 loss=4.37, loss_v1=0, loss_v2=0, nll_loss=2.307, ntokens=386.8, nsentences=32, sample_size=386.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.87748, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.838, expert1_balance_bottom=17.366, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.95, wps=96.4, ups=0.25, wpb=386.8, bsz=32, num_updates=70, lr=3.29412e-07, gnorm=7.394, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=352
2022-05-16 20:36:38 - progress_bar.py[line:272] - INFO: epoch 001:     82 / 17711 loss=4.219, loss_v1=0, loss_v2=0, nll_loss=2.128, ntokens=376.1, nsentences=32, sample_size=376.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.890357, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.447, expert1_balance_bottom=18.228, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.37, wps=92.5, ups=0.25, wpb=376.1, bsz=32, num_updates=80, lr=3.76471e-07, gnorm=6.365, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=393
2022-05-16 20:37:18 - progress_bar.py[line:272] - INFO: epoch 001:     92 / 17711 loss=4.13, loss_v1=0, loss_v2=0, nll_loss=2.049, ntokens=374.6, nsentences=32, sample_size=374.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.878514, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.377, expert1_balance_top=37.179, expert1_balance_bottom=17.742, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.14, wps=93.3, ups=0.25, wpb=374.6, bsz=32, num_updates=90, lr=4.23529e-07, gnorm=5.757, loss_scale=32, train_wall=17, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=433
2022-05-16 20:38:01 - progress_bar.py[line:272] - INFO: epoch 001:    102 / 17711 loss=4.129, loss_v1=0, loss_v2=0, nll_loss=2.036, ntokens=387, nsentences=32, sample_size=387, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.886866, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.978, expert1_balance_bottom=18.186, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.1, wps=91.3, ups=0.24, wpb=387, bsz=32, num_updates=100, lr=4.70588e-07, gnorm=6.484, loss_scale=32, train_wall=19, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=476
2022-05-16 20:38:43 - progress_bar.py[line:272] - INFO: epoch 001:    112 / 17711 loss=4.173, loss_v1=0, loss_v2=0, nll_loss=2.087, ntokens=382.2, nsentences=32, sample_size=382.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.88746, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.486, expert1_balance_bottom=18.504, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.25, wps=90.8, ups=0.24, wpb=382.2, bsz=32, num_updates=110, lr=5.17647e-07, gnorm=5.99, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=518
2022-05-16 20:39:24 - progress_bar.py[line:272] - INFO: epoch 001:    122 / 17711 loss=4.255, loss_v1=0, loss_v2=0, nll_loss=2.183, ntokens=392.9, nsentences=32, sample_size=392.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.884528, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.279, expert1_balance_bottom=17.988, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.54, wps=95.6, ups=0.24, wpb=392.9, bsz=32, num_updates=120, lr=5.64706e-07, gnorm=6.133, loss_scale=32, train_wall=17, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=559
2022-05-16 20:40:06 - progress_bar.py[line:272] - INFO: epoch 001:    132 / 17711 loss=4.195, loss_v1=0, loss_v2=0, nll_loss=2.126, ntokens=395.8, nsentences=32, sample_size=395.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.878191, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.727, expert1_balance_bottom=17.833, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.37, wps=93.3, ups=0.24, wpb=395.8, bsz=32, num_updates=130, lr=6.11765e-07, gnorm=5.771, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=601
2022-05-16 20:40:48 - progress_bar.py[line:272] - INFO: epoch 001:    142 / 17711 loss=4.111, loss_v1=0, loss_v2=0, nll_loss=2.035, ntokens=376.8, nsentences=32, sample_size=376.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.883993, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.888, expert1_balance_bottom=18.377, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.1, wps=89.8, ups=0.24, wpb=376.8, bsz=32, num_updates=140, lr=6.58824e-07, gnorm=6.102, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=643
2022-05-16 20:41:31 - progress_bar.py[line:272] - INFO: epoch 001:    152 / 17711 loss=4.084, loss_v1=0, loss_v2=0, nll_loss=1.997, ntokens=374.3, nsentences=32, sample_size=374.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.894735, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=35.518, expert1_balance_bottom=18.052, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.99, wps=88.5, ups=0.24, wpb=374.3, bsz=32, num_updates=150, lr=7.05882e-07, gnorm=5.062, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=685
2022-05-16 20:42:11 - progress_bar.py[line:272] - INFO: epoch 001:    162 / 17711 loss=4.115, loss_v1=0, loss_v2=0, nll_loss=2.04, ntokens=378.7, nsentences=32, sample_size=378.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.888482, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.723, expert1_balance_bottom=17.88, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.11, wps=94, ups=0.25, wpb=378.7, bsz=32, num_updates=160, lr=7.52941e-07, gnorm=5.548, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=726
2022-05-16 20:42:53 - progress_bar.py[line:272] - INFO: epoch 001:    172 / 17711 loss=4.16, loss_v1=0, loss_v2=0, nll_loss=2.101, ntokens=380.2, nsentences=32, sample_size=380.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.887128, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.504, expert1_balance_bottom=18.128, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.29, wps=90.3, ups=0.24, wpb=380.2, bsz=32, num_updates=170, lr=8e-07, gnorm=5.326, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=768
2022-05-16 20:43:34 - progress_bar.py[line:272] - INFO: epoch 001:    182 / 17711 loss=4.13, loss_v1=0, loss_v2=0, nll_loss=2.06, ntokens=379.7, nsentences=32, sample_size=379.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.889271, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.574, expert1_balance_bottom=18.424, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.17, wps=92.7, ups=0.24, wpb=379.7, bsz=32, num_updates=180, lr=8.47059e-07, gnorm=5.248, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=809
2022-05-16 20:44:15 - progress_bar.py[line:272] - INFO: epoch 001:    192 / 17711 loss=4.074, loss_v1=0, loss_v2=0, nll_loss=2.001, ntokens=379, nsentences=32, sample_size=379, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.887507, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.866, expert1_balance_bottom=17.582, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4, wps=92, ups=0.24, wpb=379, bsz=32, num_updates=190, lr=8.94118e-07, gnorm=4.823, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=850
2022-05-16 20:44:57 - progress_bar.py[line:272] - INFO: epoch 001:    202 / 17711 loss=4.099, loss_v1=0, loss_v2=0, nll_loss=2.035, ntokens=381.3, nsentences=32, sample_size=381.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.886598, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.753, expert1_balance_bottom=18.168, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.1, wps=90.9, ups=0.24, wpb=381.3, bsz=32, num_updates=200, lr=9.41176e-07, gnorm=5.269, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=892
2022-05-16 20:45:37 - progress_bar.py[line:272] - INFO: epoch 001:    212 / 17711 loss=4.069, loss_v1=0, loss_v2=0, nll_loss=1.991, ntokens=381.9, nsentences=32, sample_size=381.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.893897, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=35.166, expert1_balance_bottom=18.207, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.98, wps=95.2, ups=0.25, wpb=381.9, bsz=32, num_updates=210, lr=9.88235e-07, gnorm=4.688, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=932
2022-05-16 20:46:20 - progress_bar.py[line:272] - INFO: epoch 001:    222 / 17711 loss=4.12, loss_v1=0, loss_v2=0, nll_loss=2.063, ntokens=376.1, nsentences=32, sample_size=376.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.885291, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.923, expert1_balance_bottom=17.894, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.18, wps=88.8, ups=0.24, wpb=376.1, bsz=32, num_updates=220, lr=1.03529e-06, gnorm=4.556, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=974
2022-05-16 20:47:01 - progress_bar.py[line:272] - INFO: epoch 001:    232 / 17711 loss=4.071, loss_v1=0, loss_v2=0, nll_loss=2.013, ntokens=383.9, nsentences=32, sample_size=383.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.880127, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=34.831, expert1_balance_bottom=18.037, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.03, wps=92.3, ups=0.24, wpb=383.9, bsz=32, num_updates=230, lr=1.08235e-06, gnorm=4.682, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1016
2022-05-16 20:47:44 - progress_bar.py[line:272] - INFO: epoch 001:    242 / 17711 loss=3.934, loss_v1=0, loss_v2=0, nll_loss=1.86, ntokens=386.4, nsentences=32, sample_size=386.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.881125, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.493, expert1_balance_bottom=18.17, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.63, wps=90.1, ups=0.23, wpb=386.4, bsz=32, num_updates=240, lr=1.12941e-06, gnorm=4.048, loss_scale=32, train_wall=19, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1059
2022-05-16 20:48:26 - progress_bar.py[line:272] - INFO: epoch 001:    252 / 17711 loss=4.085, loss_v1=0, loss_v2=0, nll_loss=2.021, ntokens=382.8, nsentences=32, sample_size=382.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.883937, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.866, expert1_balance_bottom=18.739, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.06, wps=91.2, ups=0.24, wpb=382.8, bsz=32, num_updates=250, lr=1.17647e-06, gnorm=3.884, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1101
2022-05-16 20:49:08 - progress_bar.py[line:272] - INFO: epoch 001:    262 / 17711 loss=4.148, loss_v1=0, loss_v2=0, nll_loss=2.102, ntokens=382.7, nsentences=32, sample_size=382.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.87945, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.614, expert1_balance_bottom=18.094, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.29, wps=90.7, ups=0.24, wpb=382.7, bsz=32, num_updates=260, lr=1.22353e-06, gnorm=4.346, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1143
2022-05-16 20:49:49 - progress_bar.py[line:272] - INFO: epoch 001:    272 / 17711 loss=4.199, loss_v1=0, loss_v2=0, nll_loss=2.14, ntokens=376.1, nsentences=32, sample_size=376.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.8909, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.999, expert1_balance_bottom=18.083, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.41, wps=93.1, ups=0.25, wpb=376.1, bsz=32, num_updates=270, lr=1.27059e-06, gnorm=4.349, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1183
2022-05-16 20:50:30 - progress_bar.py[line:272] - INFO: epoch 001:    282 / 17711 loss=4.056, loss_v1=0, loss_v2=0, nll_loss=1.998, ntokens=383.1, nsentences=32, sample_size=383.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.877004, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.177, expert1_balance_bottom=18.306, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4, wps=92.3, ups=0.24, wpb=383.1, bsz=32, num_updates=280, lr=1.31765e-06, gnorm=4.337, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1225
2022-05-16 20:51:13 - progress_bar.py[line:272] - INFO: epoch 001:    292 / 17711 loss=4.192, loss_v1=0, loss_v2=0, nll_loss=2.131, ntokens=371.7, nsentences=32, sample_size=371.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.894789, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.561, expert1_balance_bottom=18.42, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.38, wps=87.4, ups=0.24, wpb=371.7, bsz=32, num_updates=290, lr=1.36471e-06, gnorm=4.833, loss_scale=32, train_wall=19, cuda_gb_allocated=11.5, cuda_gb_reserved=12.3, cuda_gb_free=12.2, wall=1267
2022-05-16 20:51:55 - progress_bar.py[line:272] - INFO: epoch 001:    302 / 17711 loss=4.103, loss_v1=0, loss_v2=0, nll_loss=2.073, ntokens=386.4, nsentences=32, sample_size=386.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.86149, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.895, expert1_balance_bottom=17.678, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.21, wps=92.4, ups=0.24, wpb=386.4, bsz=32, num_updates=300, lr=1.41176e-06, gnorm=3.878, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1309
2022-05-16 20:52:36 - progress_bar.py[line:272] - INFO: epoch 001:    312 / 17711 loss=4.169, loss_v1=0, loss_v2=0, nll_loss=2.118, ntokens=383.4, nsentences=32, sample_size=383.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.883953, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.033, expert1_balance_bottom=17.954, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.34, wps=92.5, ups=0.24, wpb=383.4, bsz=32, num_updates=310, lr=1.45882e-06, gnorm=4.094, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1351
2022-05-16 20:53:18 - progress_bar.py[line:272] - INFO: epoch 001:    322 / 17711 loss=4.067, loss_v1=0, loss_v2=0, nll_loss=2.013, ntokens=388.7, nsentences=32, sample_size=388.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.874743, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.555, expert1_balance_bottom=17.896, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.04, wps=92.7, ups=0.24, wpb=388.7, bsz=32, num_updates=320, lr=1.50588e-06, gnorm=3.76, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1393
2022-05-16 20:53:59 - progress_bar.py[line:272] - INFO: epoch 001:    332 / 17711 loss=4.136, loss_v1=0, loss_v2=0, nll_loss=2.092, ntokens=378.8, nsentences=32, sample_size=378.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.875452, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.563, expert1_balance_bottom=17.761, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.26, wps=92.1, ups=0.24, wpb=378.8, bsz=32, num_updates=330, lr=1.55294e-06, gnorm=3.634, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1434
2022-05-16 20:54:41 - progress_bar.py[line:272] - INFO: epoch 001:    342 / 17711 loss=4.109, loss_v1=0, loss_v2=0, nll_loss=2.044, ntokens=386.1, nsentences=32, sample_size=386.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.889754, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.834, expert1_balance_bottom=18.295, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.12, wps=93.1, ups=0.24, wpb=386.1, bsz=32, num_updates=340, lr=1.6e-06, gnorm=3.905, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1475
2022-05-16 20:55:22 - progress_bar.py[line:272] - INFO: epoch 001:    352 / 17711 loss=4.016, loss_v1=0, loss_v2=0, nll_loss=1.941, ntokens=381, nsentences=32, sample_size=381, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.893698, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.74, expert1_balance_bottom=18.611, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.84, wps=91.3, ups=0.24, wpb=381, bsz=32, num_updates=350, lr=1.64706e-06, gnorm=4.055, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1517
2022-05-16 20:56:04 - progress_bar.py[line:272] - INFO: epoch 001:    362 / 17711 loss=4.003, loss_v1=0, loss_v2=0, nll_loss=1.933, ntokens=382.5, nsentences=32, sample_size=382.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.886509, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=35.579, expert1_balance_bottom=18.133, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.82, wps=92.1, ups=0.24, wpb=382.5, bsz=32, num_updates=360, lr=1.69412e-06, gnorm=3.359, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1559
2022-05-16 20:56:47 - progress_bar.py[line:272] - INFO: epoch 001:    372 / 17711 loss=4.069, loss_v1=0, loss_v2=0, nll_loss=2.006, ntokens=377.7, nsentences=32, sample_size=377.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.884516, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=34.778, expert1_balance_bottom=18.807, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.02, wps=88.5, ups=0.23, wpb=377.7, bsz=32, num_updates=370, lr=1.74118e-06, gnorm=3.478, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1601
2022-05-16 20:57:28 - progress_bar.py[line:272] - INFO: epoch 001:    382 / 17711 loss=4.02, loss_v1=0, loss_v2=0, nll_loss=1.939, ntokens=379.3, nsentences=32, sample_size=379.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.891415, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=35.362, expert1_balance_bottom=18.177, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.83, wps=92, ups=0.24, wpb=379.3, bsz=32, num_updates=380, lr=1.78824e-06, gnorm=3.981, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1642
2022-05-16 20:58:09 - progress_bar.py[line:272] - INFO: epoch 001:    392 / 17711 loss=4.007, loss_v1=0, loss_v2=0, nll_loss=1.943, ntokens=385, nsentences=32, sample_size=385, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.878566, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.895, expert1_balance_bottom=17.884, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.85, wps=94.4, ups=0.25, wpb=385, bsz=32, num_updates=390, lr=1.83529e-06, gnorm=3.818, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1683
2022-05-16 20:58:51 - progress_bar.py[line:272] - INFO: epoch 001:    402 / 17711 loss=4.173, loss_v1=0, loss_v2=0, nll_loss=2.127, ntokens=383.5, nsentences=32, sample_size=383.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.87431, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.179, expert1_balance_bottom=17.817, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.37, wps=91.2, ups=0.24, wpb=383.5, bsz=32, num_updates=400, lr=1.88235e-06, gnorm=3.846, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1725
2022-05-16 20:59:33 - progress_bar.py[line:272] - INFO: epoch 001:    412 / 17711 loss=4.013, loss_v1=0, loss_v2=0, nll_loss=1.94, ntokens=383.4, nsentences=32, sample_size=383.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.890205, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.131, expert1_balance_bottom=18.197, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.84, wps=90.5, ups=0.24, wpb=383.4, bsz=32, num_updates=410, lr=1.92941e-06, gnorm=4.41, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1768
2022-05-16 21:00:15 - progress_bar.py[line:272] - INFO: epoch 001:    422 / 17711 loss=4.028, loss_v1=0, loss_v2=0, nll_loss=1.949, ntokens=377.8, nsentences=32, sample_size=377.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.893186, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.68, expert1_balance_bottom=18.869, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.86, wps=91, ups=0.24, wpb=377.8, bsz=32, num_updates=420, lr=1.97647e-06, gnorm=4.046, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1809
2022-05-16 21:00:57 - progress_bar.py[line:272] - INFO: epoch 001:    432 / 17711 loss=3.984, loss_v1=0, loss_v2=0, nll_loss=1.9, ntokens=381.2, nsentences=32, sample_size=381.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.89136, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.836, expert1_balance_bottom=18.305, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.73, wps=90.7, ups=0.24, wpb=381.2, bsz=32, num_updates=430, lr=2.02353e-06, gnorm=3.385, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1851
2022-05-16 21:01:39 - progress_bar.py[line:272] - INFO: epoch 001:    442 / 17711 loss=4.066, loss_v1=0, loss_v2=0, nll_loss=1.981, ntokens=381.6, nsentences=32, sample_size=381.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.899669, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.768, expert1_balance_bottom=18.395, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.95, wps=89.5, ups=0.23, wpb=381.6, bsz=32, num_updates=440, lr=2.07059e-06, gnorm=3.485, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1894
2022-05-16 21:02:21 - progress_bar.py[line:272] - INFO: epoch 001:    452 / 17711 loss=4.064, loss_v1=0, loss_v2=0, nll_loss=1.987, ntokens=377.2, nsentences=32, sample_size=377.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.892963, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=35.475, expert1_balance_bottom=17.898, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.96, wps=89.7, ups=0.24, wpb=377.2, bsz=32, num_updates=450, lr=2.11765e-06, gnorm=4.06, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1936
2022-05-16 21:03:02 - progress_bar.py[line:272] - INFO: epoch 001:    462 / 17711 loss=3.957, loss_v1=0, loss_v2=0, nll_loss=1.869, ntokens=389.3, nsentences=32, sample_size=389.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.89115, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=34.609, expert1_balance_bottom=18.684, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.65, wps=95.6, ups=0.25, wpb=389.3, bsz=32, num_updates=460, lr=2.16471e-06, gnorm=3.778, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=1977
2022-05-16 21:03:45 - progress_bar.py[line:272] - INFO: epoch 001:    472 / 17711 loss=4.101, loss_v1=0, loss_v2=0, nll_loss=2.019, ntokens=384.6, nsentences=32, sample_size=384.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.902481, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.335, expert1_balance_bottom=18.31, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.05, wps=89.7, ups=0.23, wpb=384.6, bsz=32, num_updates=470, lr=2.21176e-06, gnorm=3.927, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2020
2022-05-16 21:04:28 - progress_bar.py[line:272] - INFO: epoch 001:    482 / 17711 loss=4.003, loss_v1=0, loss_v2=0, nll_loss=1.922, ntokens=384.4, nsentences=32, sample_size=384.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.888772, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.753, expert1_balance_bottom=18.554, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.79, wps=90, ups=0.23, wpb=384.4, bsz=32, num_updates=480, lr=2.25882e-06, gnorm=3.674, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2062
2022-05-16 21:05:09 - progress_bar.py[line:272] - INFO: epoch 001:    492 / 17711 loss=4.096, loss_v1=0, loss_v2=0, nll_loss=2.046, ntokens=398.2, nsentences=32, sample_size=398.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.867554, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=37.045, expert1_balance_bottom=17.788, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.13, wps=95.1, ups=0.24, wpb=398.2, bsz=32, num_updates=490, lr=2.30588e-06, gnorm=3.99, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2104
2022-05-16 21:05:52 - progress_bar.py[line:272] - INFO: epoch 001:    502 / 17711 loss=4.068, loss_v1=0, loss_v2=0, nll_loss=2.005, ntokens=381.1, nsentences=32, sample_size=381.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.879739, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.173, expert1_balance_bottom=18.102, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.01, wps=89.5, ups=0.23, wpb=381.1, bsz=32, num_updates=500, lr=2.35294e-06, gnorm=3.776, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2147
2022-05-16 21:06:34 - progress_bar.py[line:272] - INFO: epoch 001:    512 / 17711 loss=4.107, loss_v1=0, loss_v2=0, nll_loss=2.038, ntokens=380.4, nsentences=32, sample_size=380.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.888609, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.965, expert1_balance_bottom=18.523, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.11, wps=91.6, ups=0.24, wpb=380.4, bsz=32, num_updates=510, lr=2.4e-06, gnorm=3.451, loss_scale=32, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2188
2022-05-16 21:07:16 - progress_bar.py[line:272] - INFO: epoch 001:    522 / 17711 loss=4.097, loss_v1=0, loss_v2=0, nll_loss=2.04, ntokens=375.7, nsentences=32, sample_size=375.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.877807, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.809, expert1_balance_bottom=18.43, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.11, wps=88.3, ups=0.23, wpb=375.7, bsz=32, num_updates=520, lr=2.44706e-06, gnorm=3.789, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2231
2022-05-16 21:07:58 - progress_bar.py[line:272] - INFO: epoch 001:    532 / 17711 loss=4.047, loss_v1=0, loss_v2=0, nll_loss=1.97, ntokens=380.5, nsentences=32, sample_size=380.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.892341, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=35.179, expert1_balance_bottom=18.281, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.92, wps=91.6, ups=0.24, wpb=380.5, bsz=32, num_updates=530, lr=2.49412e-06, gnorm=3.443, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2272
2022-05-16 21:08:40 - progress_bar.py[line:272] - INFO: epoch 001:    542 / 17711 loss=4.035, loss_v1=0, loss_v2=0, nll_loss=1.961, ntokens=383.4, nsentences=32, sample_size=383.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.888193, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=35.332, expert1_balance_bottom=18.084, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.89, wps=91, ups=0.24, wpb=383.4, bsz=32, num_updates=540, lr=2.54118e-06, gnorm=3.531, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2314
2022-05-16 21:09:23 - progress_bar.py[line:272] - INFO: epoch 001:    552 / 17711 loss=4.146, loss_v1=0, loss_v2=0, nll_loss=2.078, ntokens=379.5, nsentences=32, sample_size=379.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.889482, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=35.658, expert1_balance_bottom=18.261, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.22, wps=88.8, ups=0.23, wpb=379.5, bsz=32, num_updates=550, lr=2.58824e-06, gnorm=3.428, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2357
2022-05-16 21:10:03 - progress_bar.py[line:272] - INFO: epoch 001:    562 / 17711 loss=4.029, loss_v1=0, loss_v2=0, nll_loss=1.955, ntokens=380.5, nsentences=32, sample_size=380.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.882656, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.884, expert1_balance_bottom=18.156, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.88, wps=93.5, ups=0.25, wpb=380.5, bsz=32, num_updates=560, lr=2.63529e-06, gnorm=3.512, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2398
2022-05-16 21:10:45 - progress_bar.py[line:272] - INFO: epoch 001:    572 / 17711 loss=4.084, loss_v1=0, loss_v2=0, nll_loss=2.008, ntokens=379.5, nsentences=32, sample_size=379.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.891037, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=35.362, expert1_balance_bottom=18.533, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.02, wps=90.1, ups=0.24, wpb=379.5, bsz=32, num_updates=570, lr=2.68235e-06, gnorm=3.695, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2440
2022-05-16 21:11:27 - progress_bar.py[line:272] - INFO: epoch 001:    582 / 17711 loss=4.058, loss_v1=0, loss_v2=0, nll_loss=1.985, ntokens=380.2, nsentences=32, sample_size=380.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.885721, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=35.961, expert1_balance_bottom=18.134, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.96, wps=92.4, ups=0.24, wpb=380.2, bsz=32, num_updates=580, lr=2.72941e-06, gnorm=3.257, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2481
2022-05-16 21:12:08 - progress_bar.py[line:272] - INFO: epoch 001:    592 / 17711 loss=3.981, loss_v1=0, loss_v2=0, nll_loss=1.892, ntokens=381.7, nsentences=32, sample_size=381.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.891881, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=35.241, expert1_balance_bottom=18.301, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.71, wps=92.6, ups=0.24, wpb=381.7, bsz=32, num_updates=590, lr=2.77647e-06, gnorm=3.977, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2522
2022-05-16 21:12:49 - progress_bar.py[line:272] - INFO: epoch 001:    602 / 17711 loss=4.117, loss_v1=0, loss_v2=0, nll_loss=2.031, ntokens=376.8, nsentences=32, sample_size=376.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.903361, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.346, expert1_balance_bottom=18.494, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.09, wps=91.6, ups=0.24, wpb=376.8, bsz=32, num_updates=600, lr=2.82353e-06, gnorm=3.696, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2564
2022-05-16 21:13:32 - progress_bar.py[line:272] - INFO: epoch 001:    612 / 17711 loss=4.101, loss_v1=0, loss_v2=0, nll_loss=2.036, ntokens=388.9, nsentences=32, sample_size=388.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.883256, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.551, expert1_balance_bottom=17.921, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.1, wps=91, ups=0.23, wpb=388.9, bsz=32, num_updates=610, lr=2.87059e-06, gnorm=3.472, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2606
2022-05-16 21:14:12 - progress_bar.py[line:272] - INFO: epoch 001:    622 / 17711 loss=3.959, loss_v1=0, loss_v2=0, nll_loss=1.864, ntokens=385.5, nsentences=32, sample_size=385.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.894382, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.362, expert1_balance_bottom=18.792, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.64, wps=96.6, ups=0.25, wpb=385.5, bsz=32, num_updates=620, lr=2.91765e-06, gnorm=3.708, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2646
2022-05-16 21:14:52 - progress_bar.py[line:272] - INFO: epoch 001:    632 / 17711 loss=4.005, loss_v1=0, loss_v2=0, nll_loss=1.921, ntokens=381.9, nsentences=32, sample_size=381.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.886719, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.908, expert1_balance_bottom=18.452, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.79, wps=94.1, ups=0.25, wpb=381.9, bsz=32, num_updates=630, lr=2.96471e-06, gnorm=3.208, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2687
2022-05-16 21:15:34 - progress_bar.py[line:272] - INFO: epoch 001:    642 / 17711 loss=4.065, loss_v1=0, loss_v2=0, nll_loss=1.985, ntokens=385.2, nsentences=32, sample_size=385.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.890286, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.477, expert1_balance_bottom=18.802, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.96, wps=92.2, ups=0.24, wpb=385.2, bsz=32, num_updates=640, lr=3.01176e-06, gnorm=3.749, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2729
2022-05-16 21:16:16 - progress_bar.py[line:272] - INFO: epoch 001:    652 / 17711 loss=4.032, loss_v1=0, loss_v2=0, nll_loss=1.958, ntokens=385.3, nsentences=32, sample_size=385.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.881018, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.337, expert1_balance_bottom=18.152, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.89, wps=92.2, ups=0.24, wpb=385.3, bsz=32, num_updates=650, lr=3.05882e-06, gnorm=3.164, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2770
2022-05-16 21:16:58 - progress_bar.py[line:272] - INFO: epoch 001:    662 / 17711 loss=4.012, loss_v1=0, loss_v2=0, nll_loss=1.931, ntokens=391.9, nsentences=32, sample_size=391.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.885023, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.074, expert1_balance_bottom=18.099, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.81, wps=93.1, ups=0.24, wpb=391.9, bsz=32, num_updates=660, lr=3.10588e-06, gnorm=3.357, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2812
2022-05-16 21:17:39 - progress_bar.py[line:272] - INFO: epoch 001:    672 / 17711 loss=4.042, loss_v1=0, loss_v2=0, nll_loss=1.967, ntokens=374.1, nsentences=32, sample_size=374.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.882004, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.428, expert1_balance_bottom=17.878, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.91, wps=89.9, ups=0.24, wpb=374.1, bsz=32, num_updates=670, lr=3.15294e-06, gnorm=4.025, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2854
2022-05-16 21:18:21 - progress_bar.py[line:272] - INFO: epoch 001:    682 / 17711 loss=3.955, loss_v1=0, loss_v2=0, nll_loss=1.867, ntokens=386.2, nsentences=32, sample_size=386.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.886256, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.87, expert1_balance_bottom=18.016, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.65, wps=92.9, ups=0.24, wpb=386.2, bsz=32, num_updates=680, lr=3.2e-06, gnorm=3.419, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2896
2022-05-16 21:19:02 - progress_bar.py[line:272] - INFO: epoch 001:    692 / 17711 loss=4.05, loss_v1=0, loss_v2=0, nll_loss=1.973, ntokens=384.2, nsentences=32, sample_size=384.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.886953, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.048, expert1_balance_bottom=17.857, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.93, wps=93.2, ups=0.24, wpb=384.2, bsz=32, num_updates=690, lr=3.24706e-06, gnorm=3.676, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2937
2022-05-16 21:19:45 - progress_bar.py[line:272] - INFO: epoch 001:    702 / 17711 loss=4.104, loss_v1=0, loss_v2=0, nll_loss=2.034, ntokens=393.1, nsentences=32, sample_size=393.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.884044, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.011, expert1_balance_bottom=18.023, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.09, wps=91.8, ups=0.23, wpb=393.1, bsz=32, num_updates=700, lr=3.29412e-06, gnorm=3.449, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=2980
2022-05-16 21:20:27 - progress_bar.py[line:272] - INFO: epoch 001:    712 / 17711 loss=4.051, loss_v1=0, loss_v2=0, nll_loss=1.982, ntokens=386.6, nsentences=32, sample_size=386.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.876282, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.107, expert1_balance_bottom=18.248, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.95, wps=91.5, ups=0.24, wpb=386.6, bsz=32, num_updates=710, lr=3.34118e-06, gnorm=3.864, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3022
2022-05-16 21:21:09 - progress_bar.py[line:272] - INFO: epoch 001:    722 / 17711 loss=4.075, loss_v1=0, loss_v2=0, nll_loss=2.005, ntokens=391.4, nsentences=32, sample_size=391.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.882463, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=35.642, expert1_balance_bottom=18.103, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.01, wps=93.3, ups=0.24, wpb=391.4, bsz=32, num_updates=720, lr=3.38824e-06, gnorm=3.324, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3064
2022-05-16 21:21:51 - progress_bar.py[line:272] - INFO: epoch 001:    732 / 17711 loss=3.993, loss_v1=0, loss_v2=0, nll_loss=1.916, ntokens=383.8, nsentences=32, sample_size=383.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.878075, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.601, expert1_balance_bottom=18.351, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.77, wps=91.3, ups=0.24, wpb=383.8, bsz=32, num_updates=730, lr=3.43529e-06, gnorm=3.544, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3106
2022-05-16 21:22:33 - progress_bar.py[line:272] - INFO: epoch 001:    742 / 17711 loss=4.117, loss_v1=0, loss_v2=0, nll_loss=2.07, ntokens=384, nsentences=32, sample_size=384, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.861047, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.409, expert1_balance_bottom=17.883, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.2, wps=93.1, ups=0.24, wpb=384, bsz=32, num_updates=740, lr=3.48235e-06, gnorm=4.2, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3147
2022-05-16 21:23:14 - progress_bar.py[line:272] - INFO: epoch 001:    752 / 17711 loss=4.004, loss_v1=0, loss_v2=0, nll_loss=1.918, ntokens=386.4, nsentences=32, sample_size=386.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.88607, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.449, expert1_balance_bottom=17.938, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.78, wps=92.4, ups=0.24, wpb=386.4, bsz=32, num_updates=750, lr=3.52941e-06, gnorm=4.262, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3189
2022-05-16 21:23:57 - progress_bar.py[line:272] - INFO: epoch 001:    762 / 17711 loss=4.055, loss_v1=0, loss_v2=0, nll_loss=1.978, ntokens=379, nsentences=32, sample_size=379, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.884862, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.046, expert1_balance_bottom=18.193, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.94, wps=89.7, ups=0.24, wpb=379, bsz=32, num_updates=760, lr=3.57647e-06, gnorm=3.652, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3231
2022-05-16 21:24:39 - progress_bar.py[line:272] - INFO: epoch 001:    772 / 17711 loss=4.057, loss_v1=0, loss_v2=0, nll_loss=1.974, ntokens=384.8, nsentences=32, sample_size=384.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.886627, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.59, expert1_balance_bottom=18.177, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.93, wps=91.7, ups=0.24, wpb=384.8, bsz=32, num_updates=770, lr=3.62353e-06, gnorm=3.536, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3273
2022-05-16 21:25:20 - progress_bar.py[line:272] - INFO: epoch 001:    782 / 17711 loss=4.096, loss_v1=0, loss_v2=0, nll_loss=2.018, ntokens=377.1, nsentences=32, sample_size=377.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.890435, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.278, expert1_balance_bottom=18.386, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.05, wps=90.1, ups=0.24, wpb=377.1, bsz=32, num_updates=780, lr=3.67059e-06, gnorm=3.221, loss_scale=64, train_wall=19, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3315
2022-05-16 21:26:02 - progress_bar.py[line:272] - INFO: epoch 001:    792 / 17711 loss=4.054, loss_v1=0, loss_v2=0, nll_loss=1.99, ntokens=386.7, nsentences=32, sample_size=386.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.873559, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=37.393, expert1_balance_bottom=17.638, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.97, wps=93.2, ups=0.24, wpb=386.7, bsz=32, num_updates=790, lr=3.71765e-06, gnorm=3.805, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3357
2022-05-16 21:26:43 - progress_bar.py[line:272] - INFO: epoch 001:    802 / 17711 loss=4.1, loss_v1=0, loss_v2=0, nll_loss=2.03, ntokens=380.7, nsentences=32, sample_size=380.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.88146, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.154, expert1_balance_bottom=17.65, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.09, wps=91.7, ups=0.24, wpb=380.7, bsz=32, num_updates=800, lr=3.76471e-06, gnorm=3.325, loss_scale=64, train_wall=17, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3398
2022-05-16 21:27:25 - progress_bar.py[line:272] - INFO: epoch 001:    812 / 17711 loss=4.07, loss_v1=0, loss_v2=0, nll_loss=1.988, ntokens=384.3, nsentences=32, sample_size=384.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.892103, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=35.244, expert1_balance_bottom=18.11, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.97, wps=92.3, ups=0.24, wpb=384.3, bsz=32, num_updates=810, lr=3.81176e-06, gnorm=3.276, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3440
2022-05-16 21:28:06 - progress_bar.py[line:272] - INFO: epoch 001:    822 / 17711 loss=4.073, loss_v1=0, loss_v2=0, nll_loss=2.008, ntokens=393.1, nsentences=32, sample_size=393.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.877114, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.458, expert1_balance_bottom=17.905, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.02, wps=96.8, ups=0.25, wpb=393.1, bsz=32, num_updates=820, lr=3.85882e-06, gnorm=3.286, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3480
2022-05-16 21:28:48 - progress_bar.py[line:272] - INFO: epoch 001:    832 / 17711 loss=3.972, loss_v1=0, loss_v2=0, nll_loss=1.883, ntokens=384.2, nsentences=32, sample_size=384.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.887898, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=34.695, expert1_balance_bottom=18.549, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.69, wps=92, ups=0.24, wpb=384.2, bsz=32, num_updates=830, lr=3.90588e-06, gnorm=3.956, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3522
2022-05-16 21:29:29 - progress_bar.py[line:272] - INFO: epoch 001:    842 / 17711 loss=3.951, loss_v1=0, loss_v2=0, nll_loss=1.861, ntokens=387, nsentences=32, sample_size=387, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.881107, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.961, expert1_balance_bottom=17.853, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.63, wps=93.4, ups=0.24, wpb=387, bsz=32, num_updates=840, lr=3.95294e-06, gnorm=3.032, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3564
2022-05-16 21:30:10 - progress_bar.py[line:272] - INFO: epoch 001:    852 / 17711 loss=4.06, loss_v1=0, loss_v2=0, nll_loss=2, ntokens=393.4, nsentences=32, sample_size=393.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.867683, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.377, expert1_balance_top=36.982, expert1_balance_bottom=17.896, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4, wps=95.9, ups=0.24, wpb=393.4, bsz=32, num_updates=850, lr=4e-06, gnorm=3.472, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3605
2022-05-16 21:30:51 - progress_bar.py[line:272] - INFO: epoch 001:    862 / 17711 loss=3.961, loss_v1=0, loss_v2=0, nll_loss=1.872, ntokens=381.4, nsentences=32, sample_size=381.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.883226, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.575, expert1_balance_bottom=18.126, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.66, wps=92.1, ups=0.24, wpb=381.4, bsz=32, num_updates=860, lr=4.04706e-06, gnorm=3.784, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3646
2022-05-16 21:31:33 - progress_bar.py[line:272] - INFO: epoch 001:    872 / 17711 loss=4.032, loss_v1=0, loss_v2=0, nll_loss=1.945, ntokens=382.6, nsentences=32, sample_size=382.6, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.885788, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.799, expert1_balance_bottom=17.858, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.85, wps=92.1, ups=0.24, wpb=382.6, bsz=32, num_updates=870, lr=4.09412e-06, gnorm=3.591, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3688
2022-05-16 21:32:14 - progress_bar.py[line:272] - INFO: epoch 001:    882 / 17711 loss=4.067, loss_v1=0, loss_v2=0, nll_loss=1.989, ntokens=380.5, nsentences=32, sample_size=380.5, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.885457, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.905, expert1_balance_bottom=18.078, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.97, wps=91.6, ups=0.24, wpb=380.5, bsz=32, num_updates=880, lr=4.14118e-06, gnorm=3.668, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3729
2022-05-16 21:32:56 - progress_bar.py[line:272] - INFO: epoch 001:    892 / 17711 loss=4.07, loss_v1=0, loss_v2=0, nll_loss=2.003, ntokens=389, nsentences=32, sample_size=389, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.876181, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.377, expert1_balance_top=36.525, expert1_balance_bottom=18.116, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.01, wps=92.8, ups=0.24, wpb=389, bsz=32, num_updates=890, lr=4.18824e-06, gnorm=3.968, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3771
2022-05-16 21:33:37 - progress_bar.py[line:272] - INFO: epoch 001:    902 / 17711 loss=4.008, loss_v1=0, loss_v2=0, nll_loss=1.921, ntokens=386.4, nsentences=32, sample_size=386.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.889166, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=34.585, expert1_balance_bottom=18.12, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.79, wps=94.6, ups=0.24, wpb=386.4, bsz=32, num_updates=900, lr=4.23529e-06, gnorm=3.236, loss_scale=64, train_wall=17, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3812
2022-05-16 21:34:18 - progress_bar.py[line:272] - INFO: epoch 001:    912 / 17711 loss=4.141, loss_v1=0, loss_v2=0, nll_loss=2.076, ntokens=387.2, nsentences=32, sample_size=387.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.877943, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.114, expert1_balance_bottom=18.017, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=4.22, wps=94.6, ups=0.24, wpb=387.2, bsz=32, num_updates=910, lr=4.28235e-06, gnorm=3.826, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3853
2022-05-16 21:35:01 - progress_bar.py[line:272] - INFO: epoch 001:    922 / 17711 loss=4.018, loss_v1=0, loss_v2=0, nll_loss=1.929, ntokens=374.2, nsentences=32, sample_size=374.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.8884, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.263, expert1_balance_bottom=18.1, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.81, wps=87.9, ups=0.24, wpb=374.2, bsz=32, num_updates=920, lr=4.32941e-06, gnorm=3.87, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3895
2022-05-16 21:35:43 - progress_bar.py[line:272] - INFO: epoch 001:    932 / 17711 loss=4.01, loss_v1=0, loss_v2=0, nll_loss=1.923, ntokens=381, nsentences=32, sample_size=381, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.885922, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.453, expert1_balance_bottom=18.45, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.79, wps=90.6, ups=0.24, wpb=381, bsz=32, num_updates=930, lr=4.37647e-06, gnorm=3.385, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3937
2022-05-16 21:36:24 - progress_bar.py[line:272] - INFO: epoch 001:    942 / 17711 loss=3.986, loss_v1=0, loss_v2=0, nll_loss=1.891, ntokens=377.1, nsentences=32, sample_size=377.1, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.890182, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.562, expert1_balance_bottom=17.417, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.71, wps=90.9, ups=0.24, wpb=377.1, bsz=32, num_updates=940, lr=4.42353e-06, gnorm=3.181, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=3979
2022-05-16 21:37:06 - progress_bar.py[line:272] - INFO: epoch 001:    952 / 17711 loss=4.075, loss_v1=0, loss_v2=0, nll_loss=1.994, ntokens=378.4, nsentences=32, sample_size=378.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.88609, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.662, expert1_balance_bottom=17.845, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.98, wps=90.5, ups=0.24, wpb=378.4, bsz=32, num_updates=950, lr=4.47059e-06, gnorm=3.813, loss_scale=64, train_wall=17, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=4021
2022-05-16 21:37:47 - progress_bar.py[line:272] - INFO: epoch 001:    962 / 17711 loss=4.027, loss_v1=0, loss_v2=0, nll_loss=1.947, ntokens=380.9, nsentences=32, sample_size=380.9, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.87957, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.784, expert1_balance_bottom=18.308, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.86, wps=92.4, ups=0.24, wpb=380.9, bsz=32, num_updates=960, lr=4.51765e-06, gnorm=3.379, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=4062
2022-05-16 21:38:30 - progress_bar.py[line:272] - INFO: epoch 001:    972 / 17711 loss=3.947, loss_v1=0, loss_v2=0, nll_loss=1.86, ntokens=400.4, nsentences=32, sample_size=400.4, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.878269, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.172, expert1_balance_bottom=17.858, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.63, wps=94.5, ups=0.24, wpb=400.4, bsz=32, num_updates=970, lr=4.56471e-06, gnorm=3.395, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=4104
2022-05-16 21:39:11 - progress_bar.py[line:272] - INFO: epoch 001:    982 / 17711 loss=3.984, loss_v1=0, loss_v2=0, nll_loss=1.889, ntokens=380.3, nsentences=32, sample_size=380.3, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.892733, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=36.037, expert1_balance_bottom=18.494, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.7, wps=92.3, ups=0.24, wpb=380.3, bsz=32, num_updates=980, lr=4.61176e-06, gnorm=3.475, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=4146
2022-05-16 21:39:53 - progress_bar.py[line:272] - INFO: epoch 001:    992 / 17711 loss=3.97, loss_v1=0, loss_v2=0, nll_loss=1.885, ntokens=390.7, nsentences=32, sample_size=390.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.87938, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.377, expert1_balance_top=36.401, expert1_balance_bottom=17.775, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.69, wps=92.6, ups=0.24, wpb=390.7, bsz=32, num_updates=990, lr=4.65882e-06, gnorm=3.144, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=4188
2022-05-16 21:40:35 - progress_bar.py[line:272] - INFO: epoch 001:   1002 / 17711 loss=4.061, loss_v1=0, loss_v2=0, nll_loss=1.978, ntokens=379.8, nsentences=32, sample_size=379.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.884649, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.37, expert1_balance_bottom=18.456, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.94, wps=90.4, ups=0.24, wpb=379.8, bsz=32, num_updates=1000, lr=4.70588e-06, gnorm=3.447, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=4230
2022-05-16 21:41:16 - progress_bar.py[line:272] - INFO: epoch 001:   1012 / 17711 loss=4.069, loss_v1=0, loss_v2=0, nll_loss=1.984, ntokens=377.8, nsentences=32, sample_size=377.8, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.891073, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.056, expert1_balance_bottom=18.093, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.96, wps=91.4, ups=0.24, wpb=377.8, bsz=32, num_updates=1010, lr=4.75294e-06, gnorm=4.202, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=4271
2022-05-16 21:41:58 - progress_bar.py[line:272] - INFO: epoch 001:   1022 / 17711 loss=4.026, loss_v1=0, loss_v2=0, nll_loss=1.925, ntokens=373.2, nsentences=32, sample_size=373.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.896684, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.379, expert1_balance_top=34.998, expert1_balance_bottom=18.322, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.8, wps=89.4, ups=0.24, wpb=373.2, bsz=32, num_updates=1020, lr=4.8e-06, gnorm=3.273, loss_scale=64, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=4313
2022-05-16 21:42:40 - progress_bar.py[line:272] - INFO: epoch 001:   1032 / 17711 loss=4.016, loss_v1=0, loss_v2=0, nll_loss=1.92, ntokens=382.2, nsentences=32, sample_size=382.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.890792, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.024, expert1_balance_bottom=18.73, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.79, wps=90.5, ups=0.24, wpb=382.2, bsz=32, num_updates=1030, lr=4.84706e-06, gnorm=3.382, loss_scale=128, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=4355
2022-05-16 21:43:23 - progress_bar.py[line:272] - INFO: epoch 001:   1042 / 17711 loss=4.012, loss_v1=0, loss_v2=0, nll_loss=1.931, ntokens=375.2, nsentences=32, sample_size=375.2, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.880782, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=34.855, expert1_balance_bottom=18.453, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.81, wps=89.1, ups=0.24, wpb=375.2, bsz=32, num_updates=1040, lr=4.89412e-06, gnorm=3.52, loss_scale=128, train_wall=18, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=4397
2022-05-16 21:43:53 - progress_bar.py[line:272] - INFO: epoch 001:   1052 / 17711 loss=4.05, loss_v1=0, loss_v2=0, nll_loss=1.957, ntokens=383.7, nsentences=32, sample_size=383.7, sample_size_v1=0, sample_size_v2=0, moe_gate_loss=0.891421, overflow_expert1=0, overflow_expert2=0, entropy_gating=1.378, expert1_balance_top=35.324, expert1_balance_bottom=18.107, unused_expert1_count=0, expert2_balance_top=0, expert2_balance_bottom=0, unused_expert2_count=0, all_to_all_cpu_time_ms=0, all_to_all_cuda_time_ms=0, ppl=3.88, wps=127.1, ups=0.33, wpb=383.7, bsz=32, num_updates=1050, lr=4.94118e-06, gnorm=3.44, loss_scale=128, train_wall=17, cuda_gb_allocated=11.4, cuda_gb_reserved=12.3, cuda_gb_free=12.3, wall=4427
Killing subprocess 44394
Main process received SIGINT, exiting
